---
title: Parallel and High Performance Computing
description: 七八个星天外, 两三点雨山前. 旧时茅店社林边, 路转溪桥忽见.
date: 2021-12-01
---

* [Parallel and High Performance Computing](https://www.manning.com/books/parallel-and-high-performance-computing)

# CPU: The parallel workhorse

## Vectorization: FLOPs for free

* In the SIMD case, as the name indicates, there is
  a single instruction that is executed across
  multiple data streams.
  - One vector add instruction replaces eight individual
    scalar add instructions in the instruction queue,
    which reduces the pressure on the instruction
    queue and cache.
  - The biggest benefit is that it takes about the same
    power to perform eight additions in a vector unit
    as one scalar addition.

* Let's briefly summarize **vectorization** terminology:
  - **Vector (SIMD) lane**:
  - A pathway through a vector operation on vector
    registers for a single data element much like
    a lane on a multi-lane freeway.
  - **Vector width**
  - The width of the vector unit, usually expressed in bits.
  - **Vector length**
  - The number of data elements that can be processed
    by the vector in one operation.
  - **Vector (SIMD) instruction sets**
  - The set of instructions that extend the regular
    scalar processor instructions to utilize
    the vector processor.
* **Vectorization** is produced through both a software
  and a hardware component. The requirements are:
  - **Generate instructions**
  - The vector instructions must be generated by the
    compiler or manually specified through
    intrinsics or assembler coding.
  - **Match instructions to the vector unit of the processor**
  - If there is a mismatch between the instructions and the
    hardware, newer hardware can usually process the
    instructions, but older hardware will just fail to run.

> - When you use the latest processors, make sure to use
    the latest versions of the compiler.

* The **vector hardware releases** over the last decade
  have dramatically improved vector functionality.
  - `SSE` (Streaming SIMD Extensions)
  - `SSE2` (From `2001`)
  - `AVX` (Advanced Vector Extensions)
  - `AVX2`
  - `AVX512`
* From the years `2018` and on, **Intel** and **AMD**
  have created multiple variants of `AVX512` as
  incremental improvements to
  **vector hardware architectures**.
* There are several ways to achieve vectorization
  in your program. In ascending order of
  programmer effort, these include:
  - Optimized libraries
  - Auto-vectorization
  - Hints to the compiler
  - Vector intrinsics
  - Assembler instructions

# GPUs: Built to accelerate

## GPU architectures and concepts

* We will use the terminology established by the
  **OpenCL standard** because it was agreed to by
  multiple GPU vendors.
* We will also note alternate terminology that is
  in common use, such as that used by `NVIDIA`.
* Let's look at a few definitions before
  continuing our discussion:
  - **CPU**: The main processor that is installed
    in the socket of the motherboard.
  - **CPU RAM**: The "memory sticks" or dual in-line
    memory modules (DIMMs) containing
    *Dynamic Random-Access Memory* (**DRAM**)
    that are inserted into the memory
    slots in the motherboard.
  - **GPU**: A large peripheral card installed in
    a *Peripheral Component Interconnect Express*
    (**PCIe**) slot on the motherboard.
  - **GPU RAM**: Memory modules on the GPU peripheral
    card for exclusive use of the GPU.
  - **PCI bus**: The wiring that connects the
    peripheral cards to the other components
    on the motherboard.

* Compared to CPUs that can handle tens of
  parallel threads or processes in a clock cycle,
* GPUs are capable of processing thousands of
  parallel threads simultaneously.
* To implement algorithms on GPUs, programmers had to
  reframe their algorithms in terms of these operations,
  which was `time-consuming` and `error-prone`.
* Extending the use of the graphics processor to
  non-graphics workloads became known as general-purpose
  graphics processing unit (**GPGPU**) computing.
* The continued interest and success of `GPGPU` computing
  led to the introduction of a flurry of GPGPU languages.
  - The first to gain wide adoption was the
    Compute Unified Device Architecture (**CUDA**)
    programming language for NVIDIA GPUs, which was
    first introduced in `2007`.
  - The dominant open standard GPGPU computing language
    is the Open Computing Language (OpenCL), developed by
    a group of vendors led by Apple and released in 2009.
* GPUs come in two flavors:
  - **Integrated GPUs**: A graphics processor engine that
    is contained on the CPU
  - **Dedicated GPUs**: A GPU contained on a separate
    peripheral card

---

* In order for work to be executed on the GPU,
  at some point, data must be transferred from
  the CPU to the GPU. When the work is complete,
  and the results are going to be written to file,
  the GPU must send data back to the CPU.
* The instructions the GPU must execute are also
  sent from CPU to GPU. Each one of these transactions
  is mediated by the PCI bus. Although we won't discuss
  how to make these actions happen in this chapter,
  we'll discuss the hardware performance limitations
  of the PCI bus.
* Due to these limitations, a poorly designed GPU
  application can potentially have worse performance
  than that with CPU-only code.
* We'll also discuss the internal architecture of
  the GPU and the performance of the GPU with regards
  to memory and floating-point operations.

* For those of us who have done thread programming
  over the years on a CPU, the graphics processor is
  like the ideal thread engine. The components of
  this thread engine are:
  - A seemingly infinite number of threads
  - Zero time cost for switching or starting threads
  - Latency hiding of memory accesses through
    automatic switching between work groups

* GPU hardware replication units by vendor
  - **AMD**: Shader Engine (SE)
  - **NVIDIA/CUDA**: Graphics processing cluster
* A GPU is composed of
  - GPU RAM (also known as global memory)
  - Workload distributor
  - Compute units (CUs) (SMs in CUDA)
* **CUs** have their own internal architecture,
  often referred to as the *microarchitecture*.
* Instructions and data received from the CPU
  are processed by the workload distributor.
  The distributor coordinates instruction execution
  and data movement onto and off of the CUs.
  The achievable performance of a GPU depends on:
  - Global memory bandwidth
  - Compute unit bandwidth
  - The number of CUs
* A GPU compute device has multiple CUs.
  - **CU**, compute unit, is the term agreed to by
    the community for the OpenCL standard.
  - NVIDIA calls them *streaming multiprocessors*
    (**SMs**),
  - and Intel refers to them as *subslices*.
* Each **CU** contains multiple graphics processors
  called *processing elements* (**PEs**) in OpenCL,
  or *CUDA cores* (or *Compute Cores*) as
  NVIDIA calls them.
  - Intel refers to them as *execution units* (EUs),
  - and the graphics community calls them
    *shader processors*.
* Within each **PE**, it might be possible to perform
  an operation on more than one data item. Depending
  on the details of the GPU microprocessor architecture
  and the GPU vendor, these are referred to as
  `SIMT`, `SIMD`, or *vector operations*.
  - A similar type of functionality can be provided
    by ganging PEs together.

* A typical **GPU** has **different types of memory**.
* The list of the GPU memory types and
  their properties are as follows.
  - **Private memory (register memory)**: Immediately
    accessible by a single PE and only by that PE.
  - **Local memory**: Accessible to a single CU and
    all of the PEs on that CU. Local memory can be
    split between a scratchpad that can be used as a
    programmable cache and, by some vendors, a
    traditional cache on GPUs. Local memory is around
    `64-96` KB in size.
  - **Constant memory**: Read-only memory accessible
    and shared across all of the CUs.
  - **Global memory**: Memory that's located on the
    GPU and accessible by all of the CUs.

## GPU programming model

* Data decomposition
* Chunk-sized work for processing with some
  shared, local memory
* Operating on multiple data items with a single instruction
* Vectorization (on some GPUs)

* One thing to note from these GPU parallel abstractions
  is that there are fundamentally three, or maybe four,
  **different levels of parallelization** that you can
  apply to a computational loop.

* The technique of **data decomposition** is at the
  heart of how GPUs obtain performance. GPUs break up
  the problem into many smaller blocks of data.
  Then they break it up again, and again.
* If you only have a single instruction stream on a
  single piece of data, a GPU will be slow because it
  has no way to hide the latency. But if you have lots
  of data to operate on, it's incredibly fast.
* To further optimize the graphics operations, GPUs
  recognize that the same operations can be performed
  on many data elements.
  - GPUs are therefore optimized by working on sets of
    data with a single instruction rather than with
    separate instructions for each.
  - This reduces the number of instructions that need
    to be handled. This technique on the CPU is called
    single instruction, multiple data (SIMD).
  - All GPUs emulate this with a group of threads where
    it is called single instruction, multithread (SIMT).
* Because SIMT simulates SIMD operations, it is not
  necessarily constrained the same way as are SIMD
  operations by the underlying vector hardware.
  - Current SIMT operations are executed in lockstep,
    with every thread in the subgroup executing all
    paths through branching if any one thread must
    go through a branch.
  - This is similar to how a SIMD operation is done
    with a mask. But because the SIMT operation is
    emulated, this could be relaxed with more flexibility
    in the instruction pipeline, where more than one
    instruction could be supported.
* The basic unit of operation is called a work item
  in OpenCL. This work item can be mapped to a thread
  or to a processing core, depending on the hardware
  implementation.
* In **CUDA**, it is simply called a **thread** because
  that is how it is mapped in NVIDIA GPUs. Calling it a
  thread is mixing the programming model with how it is
  implemented in the hardware, but it is a little
  clearer to the programmer.
* A work item can invoke another level of parallelism on
  GPUs with vector hardware units. This model of operation
  also maps to the CPU where a thread can
  execute a vector operation.

---

> For convenience and generality, we call the
  **CPU** the **host** and we use the term
  **device** to refer to the **GPU**.

* The GPU programming model splits the `loop body`
  from the `array range` or `index set` that is applied
  to the function. The `loop body` creates the GPU kernel.
  The `index set` and arguments will be used on the
  host to make the kernel call.

* Extract the parallel kernel
* Map from the local data tile to global data
* Calculate data decomposition on the
  host into blocks of data
* Allocate any required memory

* GPU programming is the perfect language for
  the "Me" generation. In the kernel, everything
  is relative to yourself. Take for example

```c
c[i] = a[i] + scalar * b[i];
```

* In this expression, there is no information about the
  extent of the loop. This could be a loop where `i`,
  the global `i` index, covers a range from `0` to
  `1,000` or just the single value `22`.
* Each data item knows what needs to be done to itself
  and itself only. This is truly a "Me" programming model,
  where I care only about myself. What is so powerful
  about this is that the operations on each data element
  become completely independent.
* Let's look at the more complicated example of the
  stencil operator. Although we have two indices, both
  `i` and `j`, and some of the references are to adjacent
  data values, this line of code is still fully defined
  once we determine the values of `i` and `j`.

```c
xnew[j][i] = (x[j][i] + x[j][i-1] + x[j][i+1] + x[j-1][i] + x[j+1][i]) / 5.0;
```

* The key to how the kernel can compose its local
  operation is that, as a product of the data
  decomposition, we provide each work group
  with some information about where it is in the
  local and global domains. In OpenCL, you can get
  the following information:
  - **Dimension**: Gets the number of dimensions,
    either `1D`, `2D`, or `3D`, for this kernel
    from the kernel invocation
  - **Global information**: Global index in each
    dimension, which corresponds to a local work
    unit, or the global size in each dimension,
    which is the size of the global computational
    domain in each dimension
  - **Local (tile) information**: The local size in
    each dimension, which corresponds to the tile
    size in this dimension, or the local index in
    each dimension, which cor- responds to the
    tile index in this dimension
  - **Group information**: The number of groups in
    each dimension, which corresponds to the number
    of groups in this dimension, or the group index
    in each dimension, which corresponds to the
    group index in this dimension
* Similar information is available in CUDA, but the
  global index must be calculated from the
  local thread index plus the block (tile) information:

```c
gid = blockIdx.x *blockDim.x + threadIdx.x;
```
