---
title: (2021) 数据密集型应用系统设计
description: 江天一色无纤尘, 皎皎空中孤月轮. 江畔何人初见月? 江月何年初照人?
date: 2021-07-06
---

* [数据密集型应用系统设计](https://book.douban.com/subject/30329536/)
  - 原作名: Designing Data-Intensive Applications
  - 出版年: 2018-09-01

## 数据模型与查询语言

```
但是, 如果多对多的关系在数据中很常见呢?
关系模型能够处理简单的多对多关系,
但是随着数据之间的关联越来越复杂,
将数据建模转化为图模型会更加自然.

图由两种对象组成: 顶点 和 边 (也 称为关系)
```

> **关系**: 关联更多, 更深, 即: 更复杂

* Cypher (Neo4j)
* SPARQL
* Datalog (Prolog 子集)

## 数据存储与检索

* **Log**: 一个仅能追加的记录序列

```
日志结构的合并树
(Log-Structured Merge-Tree, 或 LSM-Tree),
它建立在更早期的日志结构文件系统之上.
因此基于合并和压缩排序文件的存储引擎通常都被称为 LSM 存储引擎.
```

```
当查找数据库中某个不存在的键时 LSM-Tree 算法可能很慢:
在确定键不存在之前, 必须先检查内存表,
然后将段一直回溯访问到最旧的段文件
(可能必须从磁盘多次读取).
为了优化这种访问, 存储引擎通常使用额外的布隆过滤器
(布隆过滤器是内存高效的数据结构, 用于近似计算集合的内容.
如果数据库中不存在某个键, 它能够很快告诉你结果,
从而节省了很多对于不存在的键的不必要的磁盘读取)
```

* LSM-tree

```
由于数据按排序存储, 因比可以有效地执行区间查询
(从最小值到最大值扫描所有的键),
并且由于磁盘是顺序写入的,
所以 LSM-tree 可以支持非常高的写入吞吐量.
```

* 对比 B-tree 和 LSM-tree

```
根据经验, LSM-tree 通常对于写人更快,
而 B-tree 被认为对于读取更快.
读取通常在 LSM-tree 上较慢,
因为它们必须在不同的压缩阶段检查多个不同的数据结构和 SSTable.
```

## 数据编码与演化

```
向后兼容:
  较新的代码 可以读取由 旧代码 编写的数据
向前兼容:
  较旧的代码 可以读取由 新代码 编写的数据
```

* OLAP 列式存储
  - 列压缩 位图编码

```
面向列的 存储压缩 和 排序 都非常有助于加速读取.
但是, 它们的缺点是让写人更加困难.
```

* Parquet

* **数据 比 代码 更长久!**
  - 哈哈!

## 数据复制

* 基于语句的复制
  - 如果遇到语句 `UPDATE ... WHERE ... NOW()`
  - 自增列, 触发器, 存储过程, UDF

* 基于预写日志 (WAL) 传输

* 基于行的逻辑日志复制
  - 变更数据捕获 (CDC)

* [Apache CouchDB](https://github.com/apache/couchdb)
  - https://couchdb.apache.org
  - Seamless multi-master syncing database
  - with an intuitive HTTP/JSON API
  - 之前看见 CouchDB, 只注意到了 `HTTP/JSON API`
  - 今天才清晰地意识到它是 **multi-master**
  - 毕竟, 也没这个需求~

* 自动冲突解决

```
冲突解决的规则可能会变得越来越复杂, 且自定义代码很容易出错.

有一些有意思的研究尝试自动解决并发修改所引起的冲突.

1. 无冲突的复制数据类型 (Conflict-free Replicated Datatypes, CRDT)

CRDT 是可以由多个用户同时编辑的数据结构, 包括 map, ordered list, 计数器等,
并且以内置的合理方式自动地解决冲突. 一些 CRDT 已经在 Riak 2.0 中得以具体实现.

2. 可合并的持久数据结构 (Mergeable persistent data)

它跟踪变更历史, 类似于 Git 版本控制系统, 并提出三向合并功能
(three-way merge function. CRDT 采用双向合并).

3. 操作转换 (Operational transformationl)

它是 Google Docs 等协作编辑应用背后的冲突解决算法.
专为可同时编辑的有序列表而设计, 如文本文档的字符列表.

这些算法总体来讲还处于早期阶段, 但将来它们可能会被整合到更多的数据系统中.
这些自动冲突解决方案可以使 主复制模型 更简单, 更容易被应用程序集成.
```

* [Introducing Riak 2.0: Data Types, Strong Consistency, Full-Text Search](https://riak.com/introducing-riak-2-0/)
  - Posted October 29, 2013
  - https://github.com/basho
  - https://github.com/basho-labs

### 复制滞后

* Dynamo (Not DynamoDB) 风格的数据存储系统经常使用以下两种机制:

```
读修复

当客户端并行读取多个副本时, 可以检测到过期的返回值.
这种方法主要适合那些被频繁读取的场景.

反熵过程

此外, 一些数据存储有后台进程不断查找副本之间数据的差异,
将任何缺少的数据从一个副本复制到另一个副本.
与基于主节点复制的复制日志不同,
此反熵过程并不保证以特定的顺序复制写入,
并且会引入明显的同步滞后.
```

* **并发性, 时间和相对性**

```
通常如果两个操作 "同时" 发生, 则称之为并发,
然而事实上, 操作是否在时间上重叠并不重要.
由于分布式系统中复杂的时钟同步问题, 现实当中,
我们很难严格确定它们是否同时发生.
为更好地定义并发性, 我们并不依赖确切的发生时间,
即不管物理的时机如何,
如果两个操作并不需要意识到对方, 我们即可声称它们是并发操作.

一些人尝试把这个思路与物理学中狭义相对论联系起来,
后者引入了 "信息传递不能超越光速" 的假定,
如果两个事件发生的间隔短于光在它们之间的折返,
那么这两个事件不可能有相互影响, 因此就是并发.

在计算机系统中, 即使光速快到允许一个操作影响到另一个操作,
但两个操作仍可能被定义为并发.
例如, 发生了网络拥塞或中断,
可能就会出现两个操作由于网络问题导致一个操作无法感知另一个,
因此二者成为并发.
```

* 三种多副本方案

```
主从复制

所有的客户端写入操作都发送到某一个节点 (主节点),
由该节点负责将数据更改事件发送到其他副本 (从节点).
每个副本都可以接收读请求, 但内容可能是过期值.

多主节点复制

系统存在多个主节点, 每个都可以接收写请求,
客户端将写请求发送到其中的一个主节点上,
由该主节点负责将数据更改事件同步到其他主节点和自己的从节点.

无主节点复制

客户端将写请求发送到多个节点上, 读取时从多个节点上并行读取,
以此检测和纠正某些过期数据.
```

```
写后读一致性

保证用户总能看到自己所提交的最新数据.

单调读

用户在某个时间点读到数据之后,
保证此后不会出现比该时间点更早的数据.

前缀一致读

保证数据之间的因果关系, 你列如, 总是以正确的顺序先读取问题, 然后看到回答.
```

## 数据分区

* 读自己的写
  - 用户 A 写入 `主`, 立即读 `从`
  - 写后读一致性/读写一致性

* 单调读

* 两种主要的分区方法

```
基于关键字区间的分区

先对关键字进行排序,
每个分区只负责一段包含最小到最大关键字范围的一段关键字.
对关键字排序的优点是可以支持高效的区间查询,
但是如果应用程序经常访问与排序一致的某段关键字,
就会存在热点的风险.
采用这种方法, 当分区太大时, 通常将其分裂为两个子区间,
从而动态地再平衡分区.

哈希分区

将哈希函数作用于每个关键字, 每个分区负责一定范围的哈希值.
这种方法打破了原关键字的顺序关系, 它的区间查询效率比较低,
但可以更均匀地分配负载. 采用哈希分区时,
通常事先创建好足够多 (但固定数量) 的分区,
让每个节点承担多个分区,
当添加或删除节点时将某些分区从一个节点迁移到另一个节点,
也可以支持动态分区.
```

```
混合上述两种基本方法也是可行的, 例如使用复合键:
键的一部分来标识分区, 而另一部分来记录排序后的顺序.
我们还讨论了分区与二级索引, 二级索引也需要进行分区,

有两种方法:

基于文档来分区二级索引 (本地索引)

二级索引存储在与关键字相同的分区中,
这意味着写入时我们只需要更新一个分区,
但缺点是读取二级索引时需要在所有分区上执行
scatter/gather.

基于词条来分区二级索引 (全局索引)

它是基于索引的值而进行的独立分区.
二级索引中的条目可能包含来自关键字的多个分区里的记录.
在写入时, 不得不更新二级索引的多个分区; 但读取时,
则可以从单个分区直接快速提取数据.
```

## 事务

### ACID

```
事务所提供的安全保证即大家所熟知的 ACID, 分别代表

原子性 (Atomicity),
一致性 (Consistency),
隔离性 (Isolation),
持久性 (Durability).

想法非常美好, 细节方见真章.
当听到一个系统声称自己 "兼容 ACID" 时,
其实你无法确信它究竟能提供了什么样的保证,
而不符合 ACID 标准的系统有时被冠以 BASE,
取另外几个特性的首字母, 即

基本可用性 (Basically Available),
软状态 (Soft state),
最终一致性 (Eventual consistency).

听下来它似乎比 ACID 更加模棱两可.
BASE 唯一可以确定的是 "它不是 ACID",
此外它几乎没有承诺任何东西.
```

* **原子性**

```
ACID 原子性 其实描述了客户端发起一个包含多个写操作的请求时可能发生的情况,
例如在完成了一部分写入之后, 系统发生了故障, 包括进程崩溃, 网络中断,
磁盘变满或者违反了某种完整性约束等; 把多个写操作纳人到一个原子事务,
万一出现了上述故障而导致没法完成最终提交时, 则事务会中止,
并且数据库须丢弃或撤销那些局部完成的更改.
假如没有原子性保证, 当多个更新操作中间发生了错误,
就需要知道哪些更改已经生效, 哪些没有生效, 这个寻找过程会非常麻烦.
或许应用程序可以重试, 但情况类似, 并且可能导致重复更新或者不正确的结果.
原子性则大大简化了这个问题: 如果事务已经中止,
应用程序可以确定没有实质发生任何更改, 所以可以安全地重试.
因此 ACID 中原子性所定义的特征是:

在出错时中止事务, 并将部分完成的写人全部丢弃.
也许可中止性比原子性更为准确.
```

* **一致性**

```
ACID 中的一致性的主要是指对数据有特定的预期状态,
任何数据更改必须满足这些状态约束 (或者恒等条件).

例如, 对于账单系统, 账户的贷款余额应和借款余额保持平衡.
如果某事务从一个有效的状态开始, 并且事务中任何更新操作都没有违背约束,
那么最后的结果依然符合有效状态.
这种一致性本质上要求应用层来维护状态一致 (或者恒等),
应用程序有责任正确地定义事务来保持一致性.
这不是数据库可以保证的事情: 即如果提供的数据修改违背了恒等条件,
数据库很难检测进而阻止该操作

(数据库可以完成针对某些特定类型的恒等约束检查,
例如使用外键约束或唯一性约束.
但通常主要靠应用程序来定义数据的有效/无效状态,
数据库主要负责存储)
```

* **隔离性**

```
ACID 语义中的 隔离性 意味着并发执行的多个事务相互隔离,
它们不能互相交叉. 经典的数据库教材把隔离定义为可串行化,
这意味着可以假装它是数据库上运行的唯一事务.
虽然实际上它们可能同时运行,
但数据库系统要确保当事务提交时,
其结果与串行执行 (一个接一个执行) 完全相同.
```

```
ACID 中的 原子性 和 隔离性
主要针对客户端在同一事务中包含多个写操作时,
数据库所提供的保证

原子性

如果一系列写操作中间发生了错误, 则事务必须中止,
并且事务中已完成的写人应该被丢弃.
换言之, 不用担心数据库的部分失败,
它总是保证要么全部成功, 要么全部失败.

隔离性

同时运行的事务不应相互干扰. 例如, 如果某个事务进行多次写人,
则另一个事务应该观察到的是其全部完成 (或者一个都没完成) 的结果,
而不应该看到中间的部分结果.
```

## 分布式系统的挑战

* 三种常见的计时系统模型

```
同步模型

同步模型假定有上界的网络延迟, 有上界的进程暂停和有上界的时钟误差.
注意, 这并不意味着完全同步的时钟或者网络延迟为零.
它只意味着你清楚地了解网络延迟, 暂停和时钟漂移不会超过某个固定的上限.
大多数实际系统的实际模型并非同步模型, 因为无限延迟和暂停确实可能发生.

部分同步模型

部分同步意味着系统在大多数情况下像一个同步系统一样运行,
但有时候会超出网络延迟, 进程暂停和时钟漂移的预期上界.
这是一个比较现实的模型:
大多数情况下, 网络和进程比较稳定 (否则几乎不可能提供持续的服务),
但是我们必须考虑到任何关于时机的假设都有偶尔违背的情况,
而一旦发生, 网络延迟, 暂停和时钟偏差可能会变得非常大.

异步模型

在这个模型中, 一个算法不会对时机做任何的假设,
甚至里面根本没有时钟 (也就没有超时机制).
某些算法可以支持纯异步模型, 但并不常见.
```

* 三种最常见的节点失效系统模型

```
崩溃-中止模型

在崩溃-中止模型中, 算法假设一个节点只能以一种方式发生故障,
即遭遇系统崩溃. 这意味着节点可能在任何时候突然停止响应,
且该节点以后永远消失, 无法恢复.

崩溃-恢复模型

节点可能会在任何时候发生崩溃且可能会在一段 (未知的) 时间之后得到恢复并再次响应.
在崩溃-恢复模型中, 节点上持久性存储 (即非易失性存储) 的数据会在崩溃之后得以保存,
而内存中状态可能会丢失.

拜占庭 (任意) 失效模型

节点可能发生任何事情, 包括试图作弊和欺骗其他节点.
对于真实系统的建模, 最普遍的组合是 崩溃-恢复模型 结合 部分同步模型.
```

## 一致性与共识

* **可线性化与可串行化**

```
可线性化 (Linearizability) 非常容易与 可串行化 (Serializability) 发生混淆,
两个词似乎都在表达类似 "可以按顺序排列" 的意思. 但是它们完全不同, 需要仔细区分:

可串行化

可串行化是事务的隔离属性, 其中每个事务可以读写多个对象 (行, 文档, 记录等).
它用来确保事务执行的结果与串行执行 (即每次执行一个事务) 的结果完全相同,
即使串行执行的顺序可能与事务实际执行顺序不同.

可线性化

可线性化是读写寄存器 (单个对象) 的最新值保证. 它并不要求将操作组合到事务中,
因此无法避免写倾斜等问题, 除非采取其他额外措施 (如实现实体化冲突).

数据库可以同时支持可串行化与线性化,
这种组合又被称为严格的可串行化或者强的单副本可串行化
(strong one copy serializability, strong-1SR)
基于两阶段加锁或者实际以串行执行都是典型的可线性化.

但是, 可串行化的快照隔离则不是线性化的:
按照设计, 它可以从一致性快照中读取, 以避免读写之间的竞争.
一致性快照的要点在于它里面不包括快照点创建时刻之后的写入数据,
因此从快照读取肯定不满足线性化.
```

* **CAP理论是否有用?**

```
CAP有时也代表一致性, 可用性, 分区容错性, 系统只能支持其中两个特性.
不过, 这种理解存在误导性. 网络分区是一种故障, 不管喜欢还是不喜欢,
它都可能发生, 所以无法选择或逃避分区的问题.

在网络正常的时候, 系统可以同时保证一致性 (线性化) 和可用性.
而一旦发生了网络故障, 必须要么选择线性一致性, 要么可用性.
因此, 更准确的称呼应该是 "网络分区情况下, 选择一致还是可用",
高可靠的网络会帮助减少发生的概率, 但无法做到彻底避免.

有必要指出, 在 CAP 的诸多讨论中, 术语可用性存在争议,
其形式化定理中的可用性与通常意义上的理解有些差别.
许多所谓的 "高可用性" (容错) 系统实际上并不符合 CAP 对可用性的特殊定义.
总之, 围绕着 CAP 有太多的误解与困扰, 最后反而无法帮助我们更好地理解系统,
所以本人建议最好避免使用 CAP.

正式定义的 CAP 定理范围很窄, 它只考虑了一种一致性模型 (即线性化)
和一种故障 (网络分区, 节点仍处于活动状态但相互断开), 而没有考虑网络延迟,
节点失败或其他需要折中的情况. 因此, 尽管 CAP 在历史上具有重大的影响力,
但对于一个具体的系统设计来说, 它可能没有太大的实际价值.
```

```
全序和偏序的差异也会体现在不同的数据库一致性模型中

可线性化

在一个可线性化的系统中, 存在全序操作关系.
系统的行为就好像只有一个数据副本, 且每个操作都是原子的,
这意味着对于任何两个操作, 我们总是可以指出哪个操作在先.

因果关系

如果两个操作都没有发生在对方之前, 那么这两个操作是并发关系.
换言之, 如果两个事件是因果关系 (一个发生在另一个之前),
那么这两个事件可以被排序; 而并发的事件则无法排序比较.
这表明因果关系至少可以定义为偏序, 而非全序.
```

* **共识的不可能性**

```
或许你可能听说过 FLP 结论, 其字母源于三位作者 Fischer, Lynch 和 Paterson.
FLP 表明如果节点存在可能崩溃的风险, 则不存在总是能够达成共识的稳定算法.
在分布式系统中, 我们必须假设节点可能会崩溃,
我们在讨论如何达成共识, 而这里又说可靠的共识无法实现. 到底怎么回事呢?

答案是 FLP 结论是基于异步系统模型而做的证明, 这是一个非常受限的模型,
它假定确定性算法都不能使用任何时钟或超时机制.
如果算法可以使用超时或其他方法来检测崩溃节点 (即使怀疑可能是误报)
那么可以实现稳定的共识方案. 另外,
即使算法使用了随机数来检测节点故障也可以绕过 FLP 结论.
因此, FLP 结论有其重要的理论意义, 但对于实际的分布式系统通常达成共识是可行的.
```

```
在这个描述中, 共识算法必须满足以下性质

协商一致性 (Uniform agreement)
  所有的节点都接受相同的决议.
诚实性 (Integrity)
  所有节点不能反悔, 即对一项提议不能有两次决定
合法性 (Validity)
  如果决定了值 v, 则 v 一定是由某个节点所提议的.
可终止性 (Termination)
  节点如果不崩溃则最终一定可以达成决议.
```

```
此外, 共识算法往往对网络间问题特别敏感.
例如, Raft 已被发现存在不合理的边界条件处理:
如果整个网络中存在某一条网络连接持续不可算,
Raft 会进入一种奇怪的状态:
它不断在两个节点之间反复切换主节点, 当前主节点不断被赶下台,
这最终导致系统根本无法安心提供服务. 其他共识算法也有类似的问题,
所以面对不可靠网络, 如何设计更具鲁棒性的共识算法仍然是一个开放性的研究问题.
```

## 批处理系统

* 以 UNIX 哲学作类比, 蛮有意思的

```
曾经:
  Online service -> 即时性的 服务 单个用户
  Batch          -> 延迟的 批量 处理多个用户
现在:
  Online service
  Streaming/批流一体

拭目以待!
```

* Bulk Synchronous Parallel (BSP) model

```
这个特点使得基于日志的消息系统更像上一章的批处理过程,
其中派生数据通过可重复的转换过程与输入数据明确分离.
它支持更多的实验性尝试, 也更容易从错误和故障中进行恢复,
从而成为集成数据流的不错选择.
```

```
事件溯源的哲学是小心的 区分事件和命令.
当来自用户的请求第一次到达时,
它最初是一个命令: 此时它可能仍然会失败,
例如因为违反了某些完整性条件.
应用程序必须首先验证它是否可以 执行该命令,
如果验证成功并且命令被接受,
它将变成一个持久且不可变的事件.
```

```
如果你擅长数学, 你可能会说应用状态是事件流对时间的积分得到的,
而变化流是状态对时间的求导得到的.
虽然这个比喻有一定的局限性
(例如状态的二阶导数似乎没有意义),
但可以帮助进一步认知数据.
```

```
事务日志记录了对数据库所做的所有更改.
高速追加是更改日志的唯一方法.
从这个角度来看, 数据库的内容保存了日志中最新记录值的缓存.
日志是事实.
数据库是日志子集的缓存,
该缓存子集恰好是来自日志的每个记录和索引值的最新值.
```

## 流处理系统

```
一个可用的复杂系统总是从可用的简单系统演进而来,
反过来这话也是正确的:
从零开始设计的复杂系统从来都用不了, 也没办法把它变成可用.
      - John Gal <系统学> (1975)
```

## 数据系统的未来

> 看看数年前的书预判未来的技术路线, 别有一番意境
