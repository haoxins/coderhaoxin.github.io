---
title: (2021) 数据密集型应用系统设计
description: 江天一色无纤尘, 皎皎空中孤月轮. 江畔何人初见月? 江月何年初照人?
date: 2021-07-06
---

* [数据密集型应用系统设计](https://book.douban.com/subject/30329536/)
  - 原作名: Designing Data-Intensive Applications
  - 出版年: 2018-09-01

## 数据模型与查询语言

```
但是, 如果多对多的关系在数据中很常见呢?
关系模型能够处理简单的多对多关系,
但是随着数据之间的关联越来越复杂,
将数据建模转化为图模型会更加自然.

图由两种对象组成: 顶点 和 边 (也 称为关系)
```

> **关系**: 关联更多, 更深, 即: 更复杂

* Cypher (Neo4j)
* SPARQL
* Datalog (Prolog 子集)

## 数据存储与检索

* **Log**: 一个仅能追加的记录序列

```
日志结构的合并树
(Log-Structured Merge-Tree, 或 LSM-Tree),
它建立在更早期的日志结构文件系统之上.
因此基于合并和压缩排序文件的存储引擎通常都被称为 LSM 存储引擎.
```

```
当查找数据库中某个不存在的键时 LSM-Tree 算法可能很慢:
在确定键不存在之前, 必须先检查内存表,
然后将段一直回溯访问到最旧的段文件
(可能必须从磁盘多次读取).
为了优化这种访问, 存储引擎通常使用额外的布隆过滤器
(布隆过滤器是内存高效的数据结构, 用于近似计算集合的内容.
如果数据库中不存在某个键, 它能够很快告诉你结果,
从而节省了很多对于不存在的键的不必要的磁盘读取)
```

* LSM-tree

```
由于数据按排序存储, 因比可以有效地执行区间查询
(从最小值到最大值扫描所有的键),
并且由于磁盘是顺序写入的,
所以 LSM-tree 可以支持非常高的写入吞吐量.
```

* 对比 B-tree 和 LSM-tree

```
根据经验, LSM-tree 通常对于写人更快,
而 B-tree 被认为对于读取更快.
读取通常在 LSM-tree 上较慢,
因为它们必须在不同的压缩阶段检查多个不同的数据结构和 SSTable.
```

## 数据编码与演化

```
向后兼容:
  较新的代码 可以读取由 旧代码 编写的数据
向前兼容:
  较旧的代码 可以读取由 新代码 编写的数据
```

* OLAP 列式存储
  - 列压缩 位图编码

```
面向列的 存储压缩 和 排序 都非常有助于加速读取.
但是, 它们的缺点是让写人更加困难.
```

* Parquet

* **数据 比 代码 更长久!**
  - 哈哈!

## 数据复制

* 基于语句的复制
  - 如果遇到语句 `UPDATE ... WHERE ... NOW()`
  - 自增列, 触发器, 存储过程, UDF

* 基于预写日志 (WAL) 传输

* 基于行的逻辑日志复制
  - 变更数据捕获 (CDC)

### 复制滞后

## 数据分区

* 读自己的写
  - 用户 A 写入 `主`, 立即读 `从`
  - 写后读一致性/读写一致性

* 单调读

## 事务

## 分布式系统的挑战

* 三种常见的计时系统模型

```
同步模型

同步模型假定有上界的网络延迟, 有上界的进程暂停和有上界的时钟误差.
注意, 这并不意味着完全同步的时钟或者网络延迟为零.
它只意味着你清楚地了解网络延迟, 暂停和时钟漂移不会超过某个固定的上限.
大多数实际系统的实际模型并非同步模型, 因为无限延迟和暂停确实可能发生.

部分同步模型

部分同步意味着系统在大多数情况下像一个同步系统一样运行,
但有时候会超出网络延迟, 进程暂停和时钟漂移的预期上界.
这是一个比较现实的模型:
大多数情况下, 网络和进程比较稳定 (否则几乎不可能提供持续的服务),
但是我们必须考虑到任何关于时机的假设都有偶尔违背的情况,
而一旦发生, 网络延迟, 暂停和时钟偏差可能会变得非常大.

异步模型

在这个模型中, 一个算法不会对时机做任何的假设,
甚至里面根本没有时钟 (也就没有超时机制).
某些算法可以支持纯异步模型, 但并不常见.
```

* 三种最常见的节点失效系统模型

```
崩溃-中止模型

在崩溃-中止模型中, 算法假设一个节点只能以一种方式发生故障,
即遭遇系统崩溃. 这意味着节点可能在任何时候突然停止响应,
且该节点以后永远消失, 无法恢复.

崩溃-恢复模型

节点可能会在任何时候发生崩溃且可能会在一段 (未知的) 时间之后得到恢复并再次响应.
在崩溃-恢复模型中, 节点上持久性存储 (即非易失性存储) 的数据会在崩溃之后得以保存,
而内存中状态可能会丢失.

拜占庭 (任意) 失效模型

节点可能发生任何事情, 包括试图作弊和欺骗其他节点.
对于真实系统的建模, 最普遍的组合是 崩溃-恢复模型 结合 部分同步模型.
```

## 一致性与共识

* **可线性化与可串行化**

```
可线性化 (Linearizability) 非常容易与 可串行化 (Serializability) 发生混淆,
两个词似乎都在表达类似 "可以按顺序排列" 的意思. 但是它们完全不同, 需要仔细区分:

可串行化

可串行化是事务的隔离属性, 其中每个事务可以读写多个对象 (行, 文档, 记录等).
它用来确保事务执行的结果与串行执行 (即每次执行一个事务) 的结果完全相同,
即使串行执行的顺序可能与事务实际执行顺序不同.

可线性化

可线性化是读写寄存器 (单个对象) 的最新值保证. 它并不要求将操作组合到事务中,
因此无法避免写倾斜等问题, 除非采取其他额外措施 (如实现实体化冲突).

数据库可以同时支持可串行化与线性化,
这种组合又被称为严格的可串行化或者强的单副本可串行化
(strong one copy serializability, strong-1SR)
基于两阶段加锁或者实际以串行执行都是典型的可线性化.

但是, 可串行化的快照隔离则不是线性化的:
按照设计, 它可以从一致性快照中读取, 以避免读写之间的竞争.
一致性快照的要点在于它里面不包括快照点创建时刻之后的写入数据,
因此从快照读取肯定不满足线性化.
```

* **CAP理论是否有用?**

```
CAP有时也代表一致性, 可用性, 分区容错性, 系统只能支持其中两个特性.
不过, 这种理解存在误导性. 网络分区是一种故障, 不管喜欢还是不喜欢,
它都可能发生, 所以无法选择或逃避分区的问题.

在网络正常的时候, 系统可以同时保证一致性 (线性化) 和可用性.
而一旦发生了网络故障, 必须要么选择线性一致性, 要么可用性.
因此, 更准确的称呼应该是 "网络分区情况下, 选择一致还是可用",
高可靠的网络会帮助减少发生的概率, 但无法做到彻底避免.

有必要指出, 在 CAP 的诸多讨论中, 术语可用性存在争议,
其形式化定理中的可用性与通常意义上的理解有些差别.
许多所谓的 "高可用性" (容错) 系统实际上并不符合 CAP 对可用性的特殊定义.
总之, 围绕着 CAP 有太多的误解与困扰, 最后反而无法帮助我们更好地理解系统,
所以本人建议最好避免使用 CAP.

正式定义的 CAP 定理范围很窄, 它只考虑了一种一致性模型 (即线性化)
和一种故障 (网络分区, 节点仍处于活动状态但相互断开), 而没有考虑网络延迟,
节点失败或其他需要折中的情况. 因此, 尽管 CAP 在历史上具有重大的影响力,
但对于一个具体的系统设计来说, 它可能没有太大的实际价值.
```

```
全序和偏序的差异也会体现在不同的数据库一致性模型中

可线性化

在一个可线性化的系统中, 存在全序操作关系.
系统的行为就好像只有一个数据副本, 且每个操作都是原子的,
这意味着对于任何两个操作, 我们总是可以指出哪个操作在先.

因果关系

如果两个操作都没有发生在对方之前, 那么这两个操作是并发关系.
换言之, 如果两个事件是因果关系 (一个发生在另一个之前),
那么这两个事件可以被排序; 而并发的事件则无法排序比较.
这表明因果关系至少可以定义为偏序, 而非全序.
```

* **共识的不可能性**

```
或许你可能听说过 FLP 结论, 其字母源于三位作者 Fischer, Lynch 和 Paterson.
FLP 表明如果节点存在可能崩溃的风险, 则不存在总是能够达成共识的稳定算法.
在分布式系统中, 我们必须假设节点可能会崩溃,
我们在讨论如何达成共识, 而这里又说可靠的共识无法实现. 到底怎么回事呢?

答案是 FLP 结论是基于异步系统模型而做的证明, 这是一个非常受限的模型,
它假定确定性算法都不能使用任何时钟或超时机制.
如果算法可以使用超时或其他方法来检测崩溃节点 (即使怀疑可能是误报)
那么可以实现稳定的共识方案. 另外,
即使算法使用了随机数来检测节点故障也可以绕过 FLP 结论.
因此, FLP 结论有其重要的理论意义, 但对于实际的分布式系统通常达成共识是可行的.
```

```
在这个描述中, 共识算法必须满足以下性质

协商一致性 (Uniform agreement)
  所有的节点都接受相同的决议.
诚实性 (Integrity)
  所有节点不能反悔, 即对一项提议不能有两次决定
合法性 (Validity)
  如果决定了值 v, 则 v 一定是由某个节点所提议的.
可终止性 (Termination)
  节点如果不崩溃则最终一定可以达成决议.
```

```
此外, 共识算法往往对网络间问题特别敏感.
例如, Raft 已被发现存在不合理的边界条件处理:
如果整个网络中存在某一条网络连接持续不可算,
Raft 会进入一种奇怪的状态:
它不断在两个节点之间反复切换主节点, 当前主节点不断被赶下台,
这最终导致系统根本无法安心提供服务. 其他共识算法也有类似的问题,
所以面对不可靠网络, 如何设计更具鲁棒性的共识算法仍然是一个开放性的研究问题.
```

## 批处理系统

* 以 UNIX 哲学作类比, 蛮有意思的

```
曾经:
  Online service -> 即时性的 服务 单个用户
  Batch          -> 延迟的 批量 处理多个用户
现在:
  Online service
  Streaming/批流一体

拭目以待!
```

* Bulk Synchronous Parallel (BSP) model

```
这个特点使得基于日志的消息系统更像上一章的批处理过程,
其中派生数据通过可重复的转换过程与输入数据明确分离.
它支持更多的实验性尝试, 也更容易从错误和故障中进行恢复,
从而成为集成数据流的不错选择.
```

```
事件溯源的哲学是小心的 区分事件和命令.
当来自用户的请求第一次到达时,
它最初是一个命令: 此时它可能仍然会失败,
例如因为违反了某些完整性条件.
应用程序必须首先验证它是否可以 执行该命令,
如果验证成功并且命令被接受,
它将变成一个持久且不可变的事件.
```

```
如果你擅长数学, 你可能会说应用状态是事件流对时间的积分得到的,
而变化流是状态对时间的求导得到的.
虽然这个比喻有一定的局限性
(例如状态的二阶导数似乎没有意义),
但可以帮助进一步认知数据.
```

```
事务日志记录了对数据库所做的所有更改.
高速追加是更改日志的唯一方法.
从这个角度来看, 数据库的内容保存了日志中最新记录值的缓存.
日志是事实.
数据库是日志子集的缓存,
该缓存子集恰好是来自日志的每个记录和索引值的最新值.
```

## 流处理系统

```
一个可用的复杂系统总是从可用的简单系统演进而来,
反过来这话也是正确的:
从零开始设计的复杂系统从来都用不了, 也没办法把它变成可用.
      - John Gal <系统学> (1975)
```

## 数据系统的未来

> 看看数年前的书预判未来的技术路线, 别有一番意境
