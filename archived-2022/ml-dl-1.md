---
title: 深度学习 (上)
description: 二十四桥仍在, 波心荡, 冷月无声. 念桥边红药, 年年知为谁生!
date: 2022-08-02
---

- [深度学习](https://book.douban.com/subject/27087503/)

> 或许对于数学基础性的研究, 会反映为 ML vs DL 的理论能力极限.
  或许吧? 未来可以无责任畅想下!

## 线性代数

- 在深度学习中, 我们也使用一些不那么常规的符号.
  我们允许矩阵和向量相加, 产生另一个矩阵:
  - $$ C = A + b $$,
    其中
    $$ C_{i, j} = A_{i, j} + b_{j} $$.
  - 换言之, 向量 `b` 和矩阵 `A` 的每一行相加.
- 这个简写方法使我们无须在加法操作前定义一个将向量 `b` 复制到每一行而生成的矩阵.
  - 这种隐式地复制向量 `b` 到很多位置的方式, 称为`广播`.

---

- 有时我们需要衡量一个向量的大小.
  - 在机器学习中, 我们经常使用称为`范数` (`norm`) 的函数来衡量向量大小.
  - 形式上,
    $$ L^p $$
    范数定义如下
  - $$ \| x \| _{p} = (\sum_{i} \mid x_i \mid ^{p})^{\frac{1}{p}} $$
  - 其中
    $$ p \in \mathbb{R} $$,
    $$ p \ge 1 $$.
- 范数 (包括
  $$ L^p $$
  范数) 是将向量映射到非负值的函数.
  - 直观上来说, 向量 `x` 的范数衡量从原点到点 `x` 的距离.
  - 更严格地说, 范数是满足下列性质的任意函数:
  - $$ f(x) = 0 \Longrightarrow x = 0 $$;
  - $$ f(x + y) \le f(x) + f(y) $$
    (__三角不等式__);
  - $$ \forall a \in \mathbb{R}, f(ax) = \mid a \mid f(x) $$.
- 当 `p = 2` 时,
  $$ L^2 $$
  范数称为欧几里得范数.
  - 它表示从原点出发到向量 `x` 确定的点的欧几里得距离.
  - $$ L^2 $$
    范数在机器学习中出现得十分频繁, 经常简化表示为
    $$ \| x \| $$,
    略去了下标 `2`.
  - 平方
    $$ L^2 $$
    范数也经常用来衡量向量的大小, 可以简单地通过点积
    $$ x^{\top} x $$
    计算.
- 平方
  $$ L^2 $$
  范数在数学和计算上都比
  $$ L^2 $$
  范数本身更方便.
  - 例如, 平方
    $$ L^2 $$
    范数对 `x` 中每个元素的导数只取决于对应的元素, 而
    $$ L^2 $$
    范数对每个元素的导数和整个向量相关.
  - 但是在很多情况下, 平方
    $$ L^2 $$
    范数也可能不受欢迎, 因为它在原点附近增长得十分缓慢.
  - 在某些机器学习应用中, 区分恰好是零的元素和非零但值很小的元素是很重要的.
  - 在这些情况下, 我们转而使用在各个位置斜率相同,
    同时保持简单的数学形式的函数:
    $$ L^1 $$
    范数.
  - $$ L^1 $$
    范数可以简化如下
  - $$ \| x \|_{1} = \sum_{i} \mid x_{i} \mid $$

---

- 另外一个经常在机器学习中出现的范数是
  $$ L^{\infty} $$
  范数, 也被称为`最大范数` (max norm).
  - 这个范数表示向量中具有最大幅值的元素的绝对值:
  - $$ \| x \|_{\infty} = \underset{i}{max} \mid x_i \mid $$
- 有时候我们可能也希望衡量矩阵的大小. 在深度学习中,
  最常见的做法是使用 `Frobenius 范数` (Frobenius norm), 即
  - $$ \| A \|_{F} = \sqrt{\sum_{i, j} A_{i, j}^2} $$
  - 其类似于向量的
    $$ L^2 $$
    范数.
- 两个向量的点积可以用范数来表示, 具体如下
  - $$ {x}^{\top}{y} = \| x \|_{2} \| y \|_{2} \cos \theta $$
  - 其中
    $$ \theta $$
    表示 `x` 和 `y` 之间的夹角.

---

- 方阵 `A` 的特征向量是指与 `A` 相乘后相当于对该向量进行缩放的非零向量 `v`:
  - $$ \mathbf{A} \mathbf{v} = \lambda \mathbf{v} $$
    其中标量
    $$ \lambda $$
    称为这个特征向量对应的特征值.
  - (类似地, 我们也可以定义左特征向量
    $$ \mathbf{v}^{\top} \mathbf{A} = \lambda \mathbf{v}^{\top} $$,
    但是通常我们更关注右特征向量).
  - 如果 `v` 是 `A` 的特征向量, 那么任何缩放后的向量
    $$ s \mathbf{v} (s \in \mathbb{R}, s \ne 0) $$
    也是 `A` 的特征向量.
  - 此外, `sv` 和 `v` 有相同的特征值.
  - 基于这个原因, 通常我们只考虑单位特征向量.
  - 假设矩阵 `A` 有 `n` 个线性无关的特征向量
    $$ \{ \mathbf{v}^{(1)}, ..., \mathbf{v}^{(n)} \} $$,
    对应着特征值
    $$ \{ \lambda_{1}, ..., \lambda_{n} \} ^{\top} $$.
  - 我们将特征向量连接成一个矩阵, 使得每一列是一个特征向量:
    $$ \mathbf{V} = \left [ \mathbf{v}^{(1)}, ..., \mathbf{v}^{(n)} \right ] $$.
  - 类似地, 我们也可以将特征值连接成一个向量
    $$ \lambda = \left [ \lambda_{1}, ..., \lambda_{n} \right ] ^{\top} $$.
  - 因此 `A` 的特征分解可以记作
    $$ \mathbf{A} = \mathbf{V} diag(\lambda) \mathbf{V}^{-1} $$

---

- 我们已经看到了构建具有特定特征值和特征向量的矩阵,
  能够使我们在目标方向上延伸空间. 然而,
  我们也常常希望将矩阵分解成特征值和特征向量.
  - 这样可以帮助我们分析矩阵的特定性质,
    就像质因数分解有助于我们理解整数.
  - 不是每一个矩阵都可以分解成特征值和特征向量.
    在某些情况下, 特征分解存在, 但是会涉及复数而非实数.
  - 幸运的是, 在本书中, 我们通常只需要分解一类有简单分解的矩阵.
- 具体来讲, 每个实对称矩阵都可以分解成实特征向量和实特征值:
  $$ \mathbf{A} = \mathbf{Q} \mathbf{Λ} \mathbf{Q} ^{\top} $$
  - 其中 `Q` 是 `A` 的特征向量组成的正交矩阵, `Λ` 是对角矩阵.
  - 特征值
    $$ Λ_{i,i} $$
    对应的特征向量是矩阵 `Q` 的第 `i` 列, 记作
    $$ Q_{:,i} $$.
  - 因为 `Q` 是正交矩阵, 我们可以将 `A` 看作沿方向
    $$ v^{(i)} $$
    延展
    $$ \lambda_{i} $$
    倍的空间.

---

## 概率与信息论

## 数值计算

## 机器学习基础

## 深度前馈网络

## 深度学习中的正则化
