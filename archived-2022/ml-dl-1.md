---
title: 深度学习 (上)
description: 二十四桥仍在, 波心荡, 冷月无声. 念桥边红药, 年年知为谁生!
date: 2022-08-02
---

- [深度学习](https://book.douban.com/subject/27087503/)

> 或许对于数学基础性的研究, 会反映为 ML vs DL 的理论能力极限.
  或许吧? 未来可以无责任畅想下!

## 线性代数

- 在深度学习中, 我们也使用一些不那么常规的符号.
  我们允许矩阵和向量相加, 产生另一个矩阵:
  - $$ C = A + b $$,
    其中
    $$ C_{i, j} = A_{i, j} + b_{j} $$.
  - 换言之, 向量 `b` 和矩阵 `A` 的每一行相加.
- 这个简写方法使我们无须在加法操作前定义一个将向量 `b` 复制到每一行而生成的矩阵.
  - 这种隐式地复制向量 `b` 到很多位置的方式, 称为`广播`.

---

- 有时我们需要衡量一个向量的大小.
  - 在机器学习中, 我们经常使用称为`范数` (`norm`) 的函数来衡量向量大小.
  - 形式上,
    $$ L^p $$
    范数定义如下
  - $$ \| x \| _{p} = (\sum_{i} \mid x_i \mid ^{p})^{\frac{1}{p}} $$
    (`2.30`)
  - 其中
    $$ p \in \mathbb{R} $$,
    $$ p \ge 1 $$.
- 范数 (包括
  $$ L^p $$
  范数) 是将向量映射到非负值的函数.
  - 直观上来说, 向量 `x` 的范数衡量从原点到点 `x` 的距离.
  - 更严格地说, 范数是满足下列性质的任意函数:
  - $$ f(x) = 0 \Longrightarrow x = 0 $$;
  - $$ f(x + y) \le f(x) + f(y) $$
    (__三角不等式__);
  - $$ \forall a \in \mathbb{R}, f(ax) = \mid a \mid f(x) $$.
- 当 `p = 2` 时,
  $$ L^2 $$
  范数称为欧几里得范数.
  - 它表示从原点出发到向量 `x` 确定的点的欧几里得距离.
  - $$ L^2 $$
    范数在机器学习中出现得十分频繁, 经常简化表示为
    $$ \| x \| $$,
    略去了下标 `2`.
  - 平方
    $$ L^2 $$
    范数也经常用来衡量向量的大小, 可以简单地通过点积
    $$ x^{\top} x $$
    计算.
- 平方
  $$ L^2 $$
  范数在数学和计算上都比
  $$ L^2 $$
  范数本身更方便.
  - 例如, 平方
    $$ L^2 $$
    范数对 `x` 中每个元素的导数只取决于对应的元素, 而
    $$ L^2 $$
    范数对每个元素的导数和整个向量相关.
  - 但是在很多情况下, 平方
    $$ L^2 $$
    范数也可能不受欢迎, 因为它在原点附近增长得十分缓慢.
  - 在某些机器学习应用中, 区分恰好是零的元素和非零但值很小的元素是很重要的.
  - 在这些情况下, 我们转而使用在各个位置斜率相同,
    同时保持简单的数学形式的函数:
    $$ L^1 $$
    范数.
  - $$ L^1 $$
    范数可以简化如下
  - $$ \| x \|_{1} = \sum_{i} \mid x_{i} \mid $$
   (`2.31`)

---

- 另外一个经常在机器学习中出现的范数是
  范数, 也被称为`最大范数` (max norm).
  - 这个范数表示向量中具有最大幅值的元素的绝对值:
    (`2.32`)
- 有时候我们可能也希望衡量矩阵的大小. 在深度学习中,
  最常见的做法是使用 `Frobenius 范数` (Frobenius norm), 即
    (`2.33`)
  - 其类似于向量的
    $$ L^2 $$
    范数.
- 两个向量的点积可以用范数来表示, 具体如下
  (`2.34`)
  - 其中
    $$ \theta $$
    表示 `x` 和 `y` 之间的夹角.

## 概率与信息论

## 数值计算

## 机器学习基础

## 深度前馈网络
