---
title: 深度学习 (中)
description: 塞下秋来风景异, 衡阳雁去无留意. 四面边声连角起, 千嶂里, 长烟落日孤城闭.
date: 2022-11-01
---

- [深度学习](https://book.douban.com/subject/27087503/)

## 卷积网络

```
卷积是一种特殊的线性运算.
卷积网络是指那些至少在网络的一层中使用卷积运算来替代一般的矩阵乘法运算的神经网络.
```

> ML 中的卷积运算和数学意义上的卷积有差异~

```
卷积运算可交换性的出现是因为我们将核相对输入进行了翻转,
从 m 增大的角度来看, 输入的索引在增大, 但是核的索引在减小.
我们将核翻转的唯一目的是实现可交换性. 尽管可交换性在证明时很有用,
但在神经网络的应用中却不是一个重要的性质. 与之不同的是,
许多神经网络库会实现一个相关的函数, 称为互相关函数,
和卷积运算几乎一样但是并没有对核进行翻转.

许多机器学习的库实现的是互相关函数但是称之为卷积.
在这本书中我们遵循把两种运算都叫作卷积的这个传统,
在与核翻转有关的上下文中, 我们会特别指明是否对核进行了翻转.
```

```
在机器学习中, 学习算法会在核合适的位置学得恰当的值,
所以一个基于核翻转的卷积运算的学习算法所学得的核,
是对未进行翻转的算法学得的核的翻转.
单独使用卷积运算在机器学习中是很少见的, 卷积经常与其他的函数一起使用,
无论卷积运算是否对它的核进行了翻转, 这些函数的组合通常是不可交换的.
```

```
处于卷积网络更深的层中的单元, 它们的接受域要比处在浅层的单元的接受域更大.
如果网络还包含类似步幅卷积或者池化之类的结构特征, 这种效应会加强.
这意味着在卷积网络中尽管直接连接都是很稀疏的,
但处在更深的层中的单元可以间接地连接到全部或者大部分输入图像.
```

```
池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出.
例如, 最大池化函数给出相邻矩形区域内的最大值.
其他常用的池化函数包括相邻矩形区域内的平均值,
L2 范数以及基于距中心像素距离的加权平均函数.
```

```
使用池化可以看作增加了一个无限强的先验:
这一层学得的函数必须具有对少量平移的不变性.
当这个假设成立时, 池化可以极大地提高网络的统计效率.
对空间区域进行池化产生了平移不变性,
但当我们对分离参数的卷积的输出进行池化时,
特征能够学得应该对于哪种变换具有不变性.
```

```
我们可以把卷积网络类比成全连接网络, 但对于这个全连接网络的权重有一个无限强的先验.
这个无限强的先验是说一个隐藏单元的权重必须和它邻居的权重相同, 但可以在空间上移动.
这个先验也要求除了那些处在隐藏单元的小的空间连续的接受域内的权重以外,
其余的权重都为零.
总之, 我们可以把卷积的使用当作对网络中一层的参数引入了一个无限强的先验概率分布.
这个先验说明了该层应该学得的函数只包含局部连接关系并且对平移具有等变性.
类似地, 使用池化也是一个无限强的先验: 每一个单元都具有对少量平移的不变性.
```

```
当我们知道每一个特征都是一小块空间的函数并且相同的特征不会出现在所有的空间上时,
局部连接层是很有用的.
```

```
平铺卷积对卷积层和局部连接层进行了折衷.
这里并不是对每一个空间位置的权重集合进行学习,
我们学习一组核使得当我们在空间移动时它们可以循环利用.
这意味着在近邻的位置上拥有不同的过滤器, 就像局部连接层一样,
但是对于这些参数的存储需求仅仅会增长常数倍,
这个常数就是核的集合的大小, 而不是整个输出的特征映射的大小.
```

- 卷积等效于使用傅里叶变换将输入与核都转换到频域, 执行两个信号的逐点相乘,
  再使用傅里叶逆变换转换回时域. 对于某些问题的规模,
  这种算法可能比离散卷积的朴素实现更快.
- 当一个 `d` 维的核可以表示成 `d` 个向量 (每一维一个向量) 的外积时,
  该核被称为可分离的. 当核可分离时, 朴素的卷积是低效的.
  它等价于组合 `d` 个一维卷积, 每个卷积使用这些向量中的一个.
  - 组合方法显著快于使用它们的外积来执行一个 `d` 维的卷积,
    并且核也只要更少的参数来表示成向量.
  - 如果核在每一维都是 `w` 个元素宽, 那么朴素的多维卷积需要
    $$ O(w^d) $$
    的运行时间和参数存储空间, 而可分离卷积只需要
    $$ O(w \times d) $$
    的运行时间和参数存储空间.
  - 当然, 并不是每个卷积都可以表示成这种形式.

## 序列建模: 循环和递归网络

## 实践方法论

## 线性因子模型

## 自编码器
