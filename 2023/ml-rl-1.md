---
title: 深度强化学习 (上)
description: 吹箫凌极浦, 日暮送夫君. 湖上一回首, 山青卷白云.
date: 2023-01-05
---

- [深度强化学习](https://book.douban.com/subject/36161659/)

- 随机性有两个来源: 动作和状态.
  动作的随机性来源于策略, 状态的随机性来源于状态转移.
  策略由策略函数决定, 状态转移由状态转移函数决定.
  - 本书中用
    $$ S_t $$
    和
    $$ s_t $$
    分别表示 `t` 时刻的状态及其观测值, 用
    $$ A_t $$
    和
    $$ a_t $$
    分别表示 `t` 时刻的动作及其观测值.

- `t` 时刻的动作价值函数
  $$ Q_{\pi} (s_t, a_t) $$
  依赖于以下三个因素.
  - 第一, 当前状态
    $$ s_t $$.
    当前状态越好,
    $$ Q_{\pi} (s_t, a_t) $$
    越大, 也就是说回报的期望值越大.
  - 第二, 当前动作
    $$ a_t $$.
    智能体执行的动作越好,
    $$ Q_{\pi} (s_t, a_t) $$
    越大.
  - 第三, 策略函数
    $$ \pi $$.
    策略决定未来的动作
    $$ A_{t+1} $$,
    $$ A_{t+2} $$,
    ...,
    $$ A_n $$
    的好坏: 策略越好,
    $$ Q_{\pi} (s_t, a_t) $$
    越大.

- 注意, 算法所需数据为四元组
  $$ (s_t, a_t, r_t, s_{t+1}) $$,
  与控制智能体运动的策略无关. 这就意味着可以用任何策略控制智能体与环境交互,
  同时记录下算法运动轨迹, 作为训练数据.
  - 因此, DQN 的训练可以分割成两个独立的部分:
  - 收集训练数据和更新参数 `w`.

- 让行为策略带有随机性的好处是能探索更多没见过的状态, 在实验中,
  初始让
  $$ \epsilon $$
  比较大 (比如
  $$ \epsilon = 0.5 $$
  ); 在训练过程中, 让
  $$ \epsilon $$
  逐渐衰减, 在几十万步之后衰减到较小的值 (比如
  $$ \epsilon = 0.01 $$
  ), 此后固定住
  $$ \epsilon = 0.01 $$.
- 异策略的好处是可以用行为策略收集经验, 把
  $$ (s_t, a_t, r_t, s_{t+1}) $$
  这样的四元组记录到一个缓存里, 事后反复利用这些经验去更新目标策略.
  - 这个缓存称为经验回放缓存, 而这种将智能体与环境交互的记录暂时保存,
    然后从中采样和学习的训练方式称为`经验回放`注意,
    经验回放只适用于异策略, 不适用于同策略,
    其原因是收集经验时用的行为策略不同于想要训练出的目标策略.

## SARSA 算法

## 价值学习高级技巧

## 策略梯度方法

## 带基线的策略梯度方法

## 策略学习高级技巧

## 连续控制

## 对状态的不完全观测
