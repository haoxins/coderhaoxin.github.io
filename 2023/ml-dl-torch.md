---
title: 动手学深度学习
description: 青山横北郭, 白水绕东城. 此地一为别, 孤蓬万里征.
date: 2023-06-24
---

- [动手学深度学习](https://book.douban.com/subject/36142067/)
  - 支持一下`李沐`

```
简而言之, 我们可以从两方面来考虑交叉熵分类目标:
(1) 最大化观测数据的似然;
(2) 最小化传达标签所需的惊异.
```

> 如果微妙的边界条件很重要, 那么我们很可能是在研究数学而非工程.

```
常见的技巧是在靠近输入层的地方设置较低的暂退概率.
```

- 一般来说, `k` 个 GPU 并行训练过程如下:
  - 在任何一次训练迭代中, 给定的随机小批量样本都将被分成 `k` 个部分,
    并均匀地分配到 GPU 上;
  - 每个 GPU 根据分配给它的小批量子集, 计算模型参数的损失和梯度;
  - 将 `k` 个 GPU 中的局部梯度聚合, 以获得当前小批量的随机梯度;
  - 聚合梯度被重新分发到每个 GPU 中;
  - 每个 GPU 使用这个小批量随机梯度, 来更新它所维护的完整的模型参数集.
- 分布式并行训练大致如下:
  - (1) 在每台机器上读取一组 (不同的) 批量数据,
    在多个 GPU 之间分割数据并传输到 GPU 的显存中.
    基于每个 GPU 上的批量数据分别计算预测和梯度.
  - (2) 来自一台机器上的所有本地 GPU 的梯度聚合在一个 GPU 上
    (或者在不同的 GPU 上聚合梯度的某些部分).
  - (3) 每台机器的梯度被发送到其本地 CPU 中.
  - (4) 所有 CPU 将梯度发送到中央参数服务器中, 由该服务器聚合所有梯度.
  - (5) 使用聚合后的梯度来更新参数, 并将更新后的参数广播回各个 CPU 中.
  - (6) 更新后的参数信息发送到本地一个 (或多个) GPU 中.
  - (7) 所有 GPU 上的参数更新完成.

## 卷积神经网络

- 在实践中, 我们很少使用不一致的步幅或填充.

```
注意, 在应用批量规范化时, 批量大小的选择可能比未批量规范化时更重要.
```

```
事实证明, 这是深度学习中一个反复出现的主题.
优化中的各种噪声源通常会导致更快的训练和较少的过拟合,
虽然目前尚未在理论上明确证明, 这种变化似乎是正则化的一种形式.
```

### ResNet (残差网络)

- 残差网络的核心思想是: 每个附加层都应该更容易地包含原始函数作为其元素之一.
  - 于是, `残差块`便诞生了.

## 循环神经网络

- 我们可以从随机偏移量开始拆分序列, 以同时获得`覆盖性`和`随机性`.
  - 下面, 我们将描述如何实现`随机抽样`和`顺序分区`策略.

```
在随机抽样中, 每个样本都是在原始的长序列上任意捕获的子序列.
在迭代过程中, 来自两个相邻的, 随机的, 小批量中的子序列不一定在原始序列中相邻.
对于语言建模, 目标是基于到目前为止我们看到的词元来预测下一个词元,
因此标签是移位了一个词元的原始序列.
```

```
在迭代过程中, 除了可以对原始序列随机抽样,
我们还可以保证两个相邻的小批量中的子序列在原始序列中也是相邻的.
这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序,
因此称为顺序分区.
```

- 遗憾的是, 虽然随机截断在理论上具有吸引力,
  但由于多种因素很可能在实践中并不比常规截断效果好,
  原因有 `3` 个方面:
  - 第一, 在对过去若干时间步经过反向传播后,
    观测结果足以捕获实际的依赖关系;
  - 第二, 增加的方差抵消了时间步数越多梯度越精确的效果;
  - 第三, 我们真正想要的是只在小范围交互的模型. 因此,
    模型需要的正是截断的通过时间反向传播方法所具备的轻度正则化效果.

- 门控循环单元与普通的循环神经网络之间的关键区别在于:
  前者支持隐状态的门控. 这意味着模型有专门的机制来确定应该何时更新隐状态,
  以及应该何时重置隐状态.

- 总之, 门控循环单元具有以下两个显著特征.
  - `重置门`有助于捕获序列中的短期依赖关系.
  - `更新门`有助于捕获序列中的长期依赖关系.

## 注意力机制

## 优化算法

## 计算机视觉

## 自然语言处理
