---
title: 动手学深度学习
description: 青山横北郭, 白水绕东城. 此地一为别, 孤蓬万里征.
date: 2023-06-24
---

- [动手学深度学习](https://book.douban.com/subject/36142067/)
  - 支持一下`李沐`

```
简而言之, 我们可以从两方面来考虑交叉熵分类目标:
(1) 最大化观测数据的似然;
(2) 最小化传达标签所需的惊异.
```

> 如果微妙的边界条件很重要, 那么我们很可能是在研究数学而非工程.

```
常见的技巧是在靠近输入层的地方设置较低的暂退概率.
```

- 一般来说, `k` 个 GPU 并行训练过程如下:
  - 在任何一次训练迭代中, 给定的随机小批量样本都将被分成 `k` 个部分,
    并均匀地分配到 GPU 上;
  - 每个 GPU 根据分配给它的小批量子集, 计算模型参数的损失和梯度;
  - 将 `k` 个 GPU 中的局部梯度聚合, 以获得当前小批量的随机梯度;
  - 聚合梯度被重新分发到每个 GPU 中;
  - 每个 GPU 使用这个小批量随机梯度, 来更新它所维护的完整的模型参数集.
- 分布式并行训练大致如下:
  - (1) 在每台机器上读取一组 (不同的) 批量数据,
    在多个 GPU 之间分割数据并传输到 GPU 的显存中.
    基于每个 GPU 上的批量数据分别计算预测和梯度.
  - (2) 来自一台机器上的所有本地 GPU 的梯度聚合在一个 GPU 上
    (或者在不同的 GPU 上聚合梯度的某些部分).
  - (3) 每台机器的梯度被发送到其本地 CPU 中.
  - (4) 所有 CPU 将梯度发送到中央参数服务器中, 由该服务器聚合所有梯度.
  - (5) 使用聚合后的梯度来更新参数, 并将更新后的参数广播回各个 CPU 中.
  - (6) 更新后的参数信息发送到本地一个 (或多个) GPU 中.
  - (7) 所有 GPU 上的参数更新完成.

## 卷积神经网络

- 在实践中, 我们很少使用不一致的步幅或填充.

```
注意, 在应用批量规范化时, 批量大小的选择可能比未批量规范化时更重要.
```

```
事实证明, 这是深度学习中一个反复出现的主题.
优化中的各种噪声源通常会导致更快的训练和较少的过拟合,
虽然目前尚未在理论上明确证明, 这种变化似乎是正则化的一种形式.
```

### ResNet (残差网络)

- 残差网络的核心思想是: 每个附加层都应该更容易地包含原始函数作为其元素之一.
  - 于是, `残差块`便诞生了.

## 循环神经网络

## 注意力机制

## 优化算法

## 计算机视觉

## 自然语言处理
