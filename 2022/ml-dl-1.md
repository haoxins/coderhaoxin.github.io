---
title: 深度学习 (上)
description: 二十四桥仍在, 波心荡, 冷月无声. 念桥边红药, 年年知为谁生!
date: 2022-08-02
---

- [深度学习](https://book.douban.com/subject/27087503/)

> 或许对于数学基础性的研究, 会反映为 ML vs DL 的理论能力极限.
  或许吧? 未来可以无责任畅想下!

## 线性代数

- 在深度学习中, 我们也使用一些不那么常规的符号.
  我们允许矩阵和向量相加, 产生另一个矩阵:
  - $$ C = A + b $$,
    其中
    $$ C_{i, j} = A_{i, j} + b_{j} $$.
  - 换言之, 向量 `b` 和矩阵 `A` 的每一行相加.
- 这个简写方法使我们无须在加法操作前定义一个将向量 `b` 复制到每一行而生成的矩阵.
  - 这种隐式地复制向量 `b` 到很多位置的方式, 称为`广播`.

---

- 有时我们需要衡量一个向量的大小.
  - 在机器学习中, 我们经常使用称为`范数` (`norm`) 的函数来衡量向量大小.
  - 形式上,
    $$ L^p $$
    范数定义如下
  - $$ \| x \| _{p} = (\sum_{i} \mid x_i \mid ^{p})^{\frac{1}{p}} $$
  - 其中
    $$ p \in \mathbb{R} $$,
    $$ p \ge 1 $$.
- 范数 (包括
  $$ L^p $$
  范数) 是将向量映射到非负值的函数.
  - 直观上来说, 向量 `x` 的范数衡量从原点到点 `x` 的距离.
  - 更严格地说, 范数是满足下列性质的任意函数:
  - $$ f(x) = 0 \Longrightarrow x = 0 $$;
  - $$ f(x + y) \le f(x) + f(y) $$
    (__三角不等式__);
  - $$ \forall a \in \mathbb{R}, f(ax) = \mid a \mid f(x) $$.
- 当 `p = 2` 时,
  $$ L^2 $$
  范数称为欧几里得范数.
  - 它表示从原点出发到向量 `x` 确定的点的欧几里得距离.
  - $$ L^2 $$
    范数在机器学习中出现得十分频繁, 经常简化表示为
    $$ \| x \| $$,
    略去了下标 `2`.
  - 平方
    $$ L^2 $$
    范数也经常用来衡量向量的大小, 可以简单地通过点积
    $$ x^{\top} x $$
    计算.
- 平方
  $$ L^2 $$
  范数在数学和计算上都比
  $$ L^2 $$
  范数本身更方便.
  - 例如, 平方
    $$ L^2 $$
    范数对 `x` 中每个元素的导数只取决于对应的元素, 而
    $$ L^2 $$
    范数对每个元素的导数和整个向量相关.
  - 但是在很多情况下, 平方
    $$ L^2 $$
    范数也可能不受欢迎, 因为它在原点附近增长得十分缓慢.
  - 在某些机器学习应用中, 区分恰好是零的元素和非零但值很小的元素是很重要的.
  - 在这些情况下, 我们转而使用在各个位置斜率相同,
    同时保持简单的数学形式的函数:
    $$ L^1 $$
    范数.
  - $$ L^1 $$
    范数可以简化如下
  - $$ \| x \|_{1} = \sum_{i} \mid x_{i} \mid $$

---

- 另外一个经常在机器学习中出现的范数是
  $$ L^{\infty} $$
  范数, 也被称为`最大范数` (max norm).
  - 这个范数表示向量中具有最大幅值的元素的绝对值:
  - $$ \| x \|_{\infty} = \underset{i}{max} \mid x_i \mid $$
- 有时候我们可能也希望衡量矩阵的大小. 在深度学习中,
  最常见的做法是使用 `Frobenius 范数` (Frobenius norm), 即
  - $$ \| A \|_{F} = \sqrt{\sum_{i, j} A_{i, j}^2} $$
  - 其类似于向量的
    $$ L^2 $$
    范数.
- 两个向量的点积可以用范数来表示, 具体如下
  - $$ {x}^{\top}{y} = \| x \|_{2} \| y \|_{2} \cos \theta $$
  - 其中
    $$ \theta $$
    表示 `x` 和 `y` 之间的夹角.

---

- 方阵 `A` 的特征向量是指与 `A` 相乘后相当于对该向量进行缩放的非零向量 `v`:
  - $$ \mathbf{A} \mathbf{v} = \lambda \mathbf{v} $$
  - 其中标量
    $$ \lambda $$
    称为这个特征向量对应的特征值.
  - (类似地, 我们也可以定义左特征向量
    $$ \mathbf{v}^{\top} \mathbf{A} = \lambda \mathbf{v}^{\top} $$,
    但是通常我们更关注右特征向量).
- 如果 `v` 是 `A` 的特征向量, 那么任何缩放后的向量
  $$ s \mathbf{v} (s \in \mathbb{R}, s \ne 0) $$
  也是 `A` 的特征向量.
  - 此外, `sv` 和 `v` 有相同的特征值.
  - 基于这个原因, 通常我们只考虑单位特征向量.
- 假设矩阵 `A` 有 `n` 个线性无关的特征向量
  $$ \{ \mathbf{v}^{(1)}, ..., \mathbf{v}^{(n)} \} $$,
  对应着特征值
  $$ \{ \lambda_{1}, ..., \lambda_{n} \} $$.
  - 我们将特征向量连接成一个矩阵, 使得每一列是一个特征向量:
    $$ \mathbf{V} = \left [ \mathbf{v}^{(1)}, ..., \mathbf{v}^{(n)} \right ] $$.
  - 类似地, 我们也可以将特征值连接成一个向量
    $$ \lambda = \left [ \lambda_{1}, ..., \lambda_{n} \right ] ^{\top} $$.
  - 因此 `A` 的特征分解可以记作
    $$ \mathbf{A} = \mathbf{V} diag(\lambda) \mathbf{V}^{-1} $$

---

- 我们已经看到了构建具有特定特征值和特征向量的矩阵,
  能够使我们在目标方向上延伸空间. 然而,
  我们也常常希望将矩阵分解成特征值和特征向量.
  - 这样可以帮助我们分析矩阵的特定性质,
    就像质因数分解有助于我们理解整数.
- 不是每一个矩阵都可以分解成特征值和特征向量.
  在某些情况下, 特征分解存在, 但是会涉及复数而非实数.
  - 幸运的是, 在本书中, 我们通常只需要分解一类有简单分解的矩阵.
- 具体来讲, 每个实对称矩阵都可以分解成实特征向量和实特征值:
  - $$ \mathbf{A} = \mathbf{Q} \mathbf{Λ} \mathbf{Q} ^{\top} $$
  - 其中 `Q` 是 `A` 的特征向量组成的正交矩阵, `Λ` 是对角矩阵.
  - 特征值
    $$ Λ_{i,i} $$
    对应的特征向量是矩阵 `Q` 的第 `i` 列, 记作
    $$ Q_{:,i} $$.
  - 因为 `Q` 是正交矩阵, 我们可以将 `A` 看作沿方向
    $$ v^{(i)} $$
    延展
    $$ \lambda_{i} $$
    倍的空间.

---

- 虽然任意一个实对称矩阵 `A` 都有特征分解, 但是特征分解可能并不唯一.
  - 如果两个或多个特征向量拥有相同的特征值,
    那么在由这些特征向量产生的生成子空间中,
    任意一组正交向量都是该特征值对应的特征向量.
- 因此, 我们可以等价地从这些特征向量中构成 `Q` 作为替代.
  - 按照惯例, 我们通常按降序排列 `Λ` 的元素.
  - 在该约定下, 特征分解唯一, 当且仅当所有的特征值都是唯一的.
  - 矩阵的特征分解给了我们很多关于矩阵的有用信息.

---

- 矩阵是奇异的, 当且仅当含有零特征值.
  实对称矩阵的特征分解也可以用于优化二次方程
  $$ f(x) = x^{\top} \mathbf{A} x $$,
  其中限制
  $$ \| x \|_{2} = 1 $$.
  - 当 `x` 等于 `A` 的某个特征向量时, `f` 将返回对应的特征值.
  - 在限制条件下, 函数 `f` 的最大值是最大特征值, 最小值是最小特征值.
- 所有特征值都是正数的矩阵称为`正定`;
  所有特征值都是非负数的矩阵称为`半正定`.
  - 同样地, 所有特征值都是负数的矩阵称为`负定`;
    所有特征值都是非正数的矩阵称为`半负定`.
- 半正定矩阵受到关注是因为它们保证
  $$ \forall x, x^{\top} \mathbf{A} x ≥ 0 $$.
  - 此外, 正定矩阵还保证
    $$ x^{\top} \mathbf{A} x = 0 \Rightarrow x = 0 $$.

---

- 奇异值分解 (singular value decomposition, SVD),
  是将矩阵分解为奇异向量 (singular vector) 和奇异值 (singular value).
  - 通过奇异值分解, 我们会得到一些与特征分解相同类型的信息.
  - 然而, 奇异值分解有更广泛的应用.
  - 每个实数矩阵都有一个奇异值分解, 但不一定都有特征分解.
  - 例如, 非方阵的矩阵没有特征分解, 这时我们只能使用奇异值分解.
- 回想一下, 我们使用特征分解去分析矩阵 `A` 时,
  得到特征向量构成的矩阵 `V` 和特征值构成的向量
  $$ \lambda $$,
  我们可以重新将 `A` 写作
  - $$ \mathbf{A} = \mathbf{V} diag(\lambda) \mathbf{V}^{-1} $$
- 奇异值分解是类似的, 只不过这回我们将矩阵 `A` 分解成三个矩阵的乘积:
  - $$ \mathbf{A} = \mathbf{U} \mathbf{D} \mathbf{V} ^{\top} $$
  - 假设 `A` 是一个
    $$ m \times n $$
    的矩阵, 那么 `U` 是一个
    $$ m \times m $$
    的矩阵, `D` 是一个
    $$ m \times n $$
    的矩阵, `V` 是一个
    $$ n \times n $$
    矩阵.
- 这些矩阵中的每一个经定义后都拥有特殊的结构.
  矩阵 `U` 和 `V` 都定义为正交矩阵,
  而矩阵 `D` 定义为对角矩阵.
  - 注意, 矩阵 `D` 不一定是方阵.
- 对角矩阵 `D` 对角线上的元素称为矩阵 `A` 的奇异值 (singular value).
  - 矩阵 `U` 的列向量称为`左奇异向量` (left singular vector),
    矩阵 `V` 的列向量称`右奇异向量` (right singular vector).
- 事实上, 我们可以用与 `A` 相关的特征分解去解释 `A` 的奇异值分解.
  - `A` 的`左奇异向量` (left singular vector) 是
    $$ \mathbf{A} \mathbf{A} ^{\top} $$
    的特征向量.
  - `A` 的`右奇异向量` (right singular vector) 是
    $$ \mathbf{A} ^{\top} \mathbf{A} $$
    的特征向量.
  - `A` 的非零奇异值是
    $$ \mathbf{A} \mathbf{A} ^{\top} $$
    特征值的平方根, 同时也是
    $$ \mathbf{A} ^{\top} \mathbf{A} $$
    特征值的平方根.

> `SVD` 最有用的一个性质可能是拓展矩阵求逆到非方矩阵上.

---

- 对于非方矩阵而言, 其逆矩阵没有定义.
  假设在下面的问题中, 我们希望通过矩阵 `A` 的左逆
  `B` 来求解线性方程:
  - $$ \mathbf{A} x = y $$
- 等式两边左乘左逆 `B` 后, 我们得到
  - $$ x = \mathbf{B} y $$
- 取决于问题的形式, 我们可能无法设计一个唯一的映射将 `A` 映射到 `B`.
  - 如果矩阵 `A` 的行数大于列数, 那么上述方程可能没有解.
  - 如果矩阵 `A` 的行数小于列数, 那么上述矩阵可能有多个解.

---

- __伪逆__ 使我们在这类问题上取得了一定的进展. 矩阵 `A` 的伪逆定义为

$$
\mathbf{A}^{+} = \lim_{a \searrow 0}
(\mathbf{A}^{\top} \mathbf{A} + a \mathbf{I})^{-1}
\mathbf{A}^{\top}
$$

- 计算伪逆的实际算法没有基于这个定义, 而是使用下面的公式
  - $$ \mathbf{A}^{+} = \mathbf{V} \mathbf{D}^{+} \mathbf{U}^{\top} $$
  - 其中, 矩阵 `U`, `D` 和 `V` 是矩阵 `A` 奇异值分解后得到的矩阵.
  - 对角矩阵 `D` 的伪逆
    $$ D^{+} $$
    是其非零元素取倒数之后再转置得到的.
- 当矩阵 `A` 的列数多于行数时, 使用伪逆求解线性方程是众多可能解法中的一种.
  - 特别地,
    $$ x = \mathbf{A}^{+} y $$
    是方程所有可行解中欧几里得范数
    $$ \| x \|_{2} $$
    最小的一个.
- 当矩阵 `A` 的行数多于列数时, 可能没有解. 在这种情况下, 通过伪逆得到的
  `x` 使得 `Ax` 和 `y` 的欧几里得距离
  $$ \| \mathbf{A} x - y \|_{2} $$
  最小.

---

- 迹运算返回的是矩阵对角元素的和:
  - $$ Tr(A) = \sum_{i} A_{i,i} $$.
- 迹运算提供了另一种描述矩阵 Frobenius 范数的方式:
  - $$ \| A \|_{F} = \sqrt{Tr(A A^{\top})} $$
- 多个矩阵相乘得到的方阵的迹,
  和将这些矩阵中的最后一个挪到最前面之后相乘的迹是相同的.
  - 当然, 我们需要考虑挪动之后矩阵乘积依然定义良好:
  - $$ Tr(ABC) = Tr(CAB) = Tr(BCA) $$
- 或者更一般地,

$$
Tr(\prod_{i = 1}^{n} F^{(i)}) =
Tr(F^{(n)} \prod_{i = 1}^{n - 1} F^{(i)})
$$

- 即使循环置换后矩阵乘积得到的矩阵形状变了,
  迹运算的结果依然不变.

## 概率与信息论

- 期望是线性的, 例如,

$$
\mathbb{E}_{x} \left [ α f(x) + β g(x) \right ] =
α \mathbb{E}_{x} \left [ f(x) \right ] +
β \mathbb{E}_{x} \left [ g(x) \right ]
$$

- 其中 `α` 和 `β` 不依赖于 `x`.

---

- `方差`衡量的是当我们对 `x` 依据它的概率分布进行采样时,
  随机变量 `x` 的函数值会呈现多大的差异:

$$
Var(f(x)) =
\mathbb{E} \left [ (
  f(x) - \mathbb{E} \left [ f(x) \right ]
)^{2} \right ]
$$

- 当方差很小时, `f(x)` 的值形成的簇比较接近它们的期望值.
  - 方差的平方根被称为`标准差`.

---

## 数值计算

## 机器学习基础

## 深度前馈网络

## 深度学习中的正则化
