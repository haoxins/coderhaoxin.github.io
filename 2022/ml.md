---
title: 机器学习 - 西瓜, 南瓜
description: 晚年惟好静, 万事不关心. 自顾无长策, 空知返旧林.
date: 2022-07-31
---

- [机器学习](https://book.douban.com/subject/26708119/)

- 任意实矩阵
  $$ A \in \mathbb{R}^{m \times n} $$
  都可分解为
  - $$ A = U \Sigma V^{T} $$,
    (`A.33`)
  - 其中,
    $$ U \in \mathbb{R}^{m \times m} $$
    是满足
    $$ U^{T} U = I $$
    的 `m` 阶酉矩阵;
  - $$ V \in \mathbb{R}^{n \times n} $$
    是满足
    $$ V^{T} V = I $$
    的 `n` 阶酉矩阵;
  - $$ \Sigma \in \mathbb{R}^{m \times n} $$
    是
    $$ m \times n $$
    的矩阵,
  - 其中
    $$ (\Sigma)_{ii} = \sigma_{i} $$
    且其他位置的元素均为 `0`,
    $$ \sigma_{i} $$
    为非负实数且满足
    $$ \sigma_1 \ge \sigma_2 \ge ... \ge 0 $$.
- 式 (`A.33`) 中的分解称为奇异值分解 (简称 `SVD`), 其中 `U` 的列向量
  $$ u_{i} \in \mathbb{R}^{m} $$
  称为 `A` 的左奇异向量, `V` 的列向量
  $$ v_{i} \in \mathbb{R}^{n} $$
  称为 `A` 的右奇异向量,
  $$ \sigma_{i} $$
  称为奇异值.
  - 矩阵 `A` 的秩 (`rank`) 就等于非零奇异值的个数.

## 模型评估与选择

- 我们根据学习器的预测结果对样例进行排序, 按此顺序逐个把样本作为正例进行预测,
  每次计算出两个重要量的值, 分别以它们为横, 纵坐标作图,
  就得到了`"ROC 曲线"`.
- 与 `P-R` 曲线使用查准率, 查全率为纵, 横轴不同,
  `ROC 曲线`的纵轴是`"真正例率"` (True Positive Rate, 简称 `TPR`),
  横轴是`"假正例率"` (False Positive Rate, 简称 `FPR`),
  两者分别定义为

$$
TPR = \frac{TP}{TP + FN}
$$

$$
FPR = \frac{FP}{TN + FP}
$$

```
也就是说, 泛化误差可分解为偏差, 方差与噪声之和.
回顾偏差, 方差, 噪声的含义:
偏差度量了学习算法的期望预测与真实结果的偏离程度,
即刻画了学习算法本身的拟合能力;
方差度量了同样大小的训练集的变动所导致的学习性能的变化,
即刻画了数据扰动所造成的影响;
噪声则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界,
即刻画了学习问题本身的难度.

偏差-方差分解说明, 泛化性能是由学习算法的能力,
数据的充分性以及学习任务本身的难度所共同决定的.
给定学习任务, 为了取得好的泛化性能, 则需使偏差较小,
即能够充分拟合数据, 并且使方差较小, 即使得数据扰动产生的影响小.

一般来说, 偏差与方差是有冲突的, 这称为偏差-方差窘境.
给定学习任务, 假定我们能控制学习算法的训练程度,
则在训练不足时, 学习器的拟合能力不够强,
训练数据的扰动不足以使学习器产生显著变化,
此时偏差主导了泛化错误率; 随着训练程度的加深,
学习器的拟合能力逐渐增强, 训练数据发生的扰动渐渐能被学习器学到,
方差逐渐主导了泛化错误率; 在训练程度充足后,
学习器的拟合能力已非常强, 训练数据发生的轻微扰动都会导致学习器发生显著变化,
若训练数据自身的, 非全局的特性被学习器学到了, 则将发生过拟合.
```

## 支持向量机

- __表示定理__ 令
  $$ \mathbb{H} $$
  为核函数
  $$ \mathcal{k} $$
  对应的再生核希尔伯特空间,
  $$ \| h \|_{\mathbb{H}} $$
  表示
  $$ \mathbb{H} $$
  空间中关于 `h` 的范数, 对于任意单调递增函数
  $$ Ω : \left [ 0, \infty \right ] \to \mathbb{R} $$
  和任意非负损失函数
  $$ \ell : \mathbb{R}^{m} \to \left [ 0, \infty \right ] $$,
  优化问题

$$
\underset{h \in \mathbb{H}}{min} F(h) =
Ω(\| h \|_{\mathbb{H}}) +
\ell (h(x_1), h(x_2), ..., h(x_m))
$$

- 的解总可写为

$$
h^{*}(x) = \sum_{i = 1}^{m} a_{i} \mathcal{k} (x, x_i)
$$

- 表示定理对损失函数没有限制, 对正则化项
  $$ Ω $$
  仅要求单调递增, 甚至不要求
  $$ Ω $$
  是凸函数, 意味着对于一般的损失函数和正则化项, 优化问题的最优解
  $$ h^{*}(x) $$
  都可表示为核函数
  $$ \mathcal{κ} (x, x_i) $$
  的线性组合; 这显示出核函数的巨大威力.

## 集成学习

```
根据个体学习器的生成方式, 目前的集成学习方法大致可分为两大类,
即个体学习器间存在强依赖关系, 必须串行生成的序列化方法,
以及个体学习器间不存在强依赖关系, 可同时生成的并行化方法;
前者的代表是 Boosting, 后者的代表是 Bagging 和 "随机森林".
```

```
事实上, 概率模型的训练过程就是参数估计过程.
对于参数估计, 统计学界的两个学派分别提供了不同的解决方案:
频率主义学派认为参数虽然未知, 但却是客观存在的固定值,
因此, 可通过优化似然函数等准则来确定参数值;
贝叶斯学派则认为参数是未观察到的随机变量, 其本身也可有分布,
因此, 可假定参数服从一个先验分布,
然后基于观测到的数据来计算参数的后验分布.
```

## 特征选择与稀疏学习

```
第一个环节是"子集搜索"问题.
给定特征集合 {a1, a2, ..., ad}, 我们可将每个特征看作一个候选子集,
对这 d 个候选单特征子集进行评价, 假定 {a2} 最优, 于是将 {a2} 作为第一轮的选定集;
然后, 在上一轮的选定集中加入一个特征, 构成包含两个特征的候选子集,
假定在这 d-1 个候选两特征子集中 {a2, a4} 最优, 且优于 {a2},
于是将 {a2, a4} 作为本轮的选定集; ... 假定在第 k+1 轮时, 最优的候选 (k+1)
特征子集不如上一轮的选定集, 则停止生成候选子集,
并将上一轮选定的 k 特征集合作为特征选择结果.

这样逐渐增加相关特征的策略称为"前向"搜索. 类似的, 若我们从完整的特征集合开始,
每次尝试去掉一个无关特征, 这样逐渐减少特征的策略称为"后向"搜索.
还可将前向与后向搜索结合起来, 每一轮逐渐增加选定相关特征
(这些特征在后续轮中将确定不会被去除), 同时减少无关特征,
这样的策略称为"双向"搜索.
```

```
将特征子集搜索机制与子集评价机制相结合, 即可得到特征选择方法.
例如将前向搜索与信息熵相结合, 这显然与决策树算法非常相似.
事实上, 决策树可用于特征选择, 树结点的划分属性所组成的集合就是选择出的特征子集.
其他的特征选择方法未必像决策树特征选择这么明显,
但它们在本质上都是显式或隐式地结合了某种 (或多种) 子集搜索机制和子集评价机制.
常见的特征选择方法大致可分为三类:
过滤式 (filter), 包裹式 (wrapper) 和嵌入式 (embedding).
```

```
在过滤式和包裹式特征选择方法中, 特征选择过程与学习器训练过程有明显的分别;
与此不同, 嵌入式特征选择是将特征选择过程与学习器训练过程融为一体,
两者在同一个优化过程中完成, 即在学习器训练过程中自动地进行了特征选择.
```

> 总体上, 全书偏概览性介绍~ 不值得推荐!

------------------

- [机器学习公式详解](https://book.douban.com/subject/35381195/)
