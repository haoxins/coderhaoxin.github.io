---
title: 数据库系统内幕
description: 少焉, 月出于东山之上, 徘徊于斗牛之间. 白露横江, 水光接天.
date: 2022-03-11
---

* [数据库系统内幕](https://book.douban.com/subject/35078474/)
  - 作为**微信读书**付费会员, 为微信读书的`时效性`提升点赞
  - 个人觉得本书优于
    [数据密集型应用系统设计](https://book.douban.com/subject/30329536/)
  - 但整体依旧是偏**综述**类, 最大的亮点是翻译的好

---

## 存储引擎简介

* **空间局部性**原则是**局部性原则**之一. 该原则指出,
  如果访问一处存储, 则其附近的其他存储区域也会在不久的将来被访问.

* 认为`行存储`和`列存储`之间的区别仅在于数据的存储方式有所不同, 这是不充分的.
  选择数据布局只是列式存储所针对的一系列可能的优化的步骤之一.
  - 在一次读取中, 从同一列中读取多个值可以显著提高缓存利用率和计算效率.
    在现代 `CPU` 上, 向量化指令可以使单条 `CPU` 指令一次处理多个数据点.
  - 另外, 将具有相同数据类型的值存储在一起可以提高压缩率.
    我们可以根据不同的数据类型使用不同的压缩算法,
    并为每种情况选择最有效的压缩方法.

* 面向列的数据库不应与宽列式存储 (如 `BigTable` 或 `HBase`) 相混淆.
  在这些数据库中, 数据表示为多维映射, 列被分组为列族 (通常存储相同类型的数据),
  并且在每个列族中, 数据被逐行存储.
  - 此布局最适合存储由一个键或一组键来检索的数据.

* 索引文件的大小通常比数据文件小. 文件被划分成页 (`page`),
  每个页通常具有单个或多个磁盘块的大小.
  - 页可以被组织成记录的序列或分槽页 (`slotted page`).
* 新增记录 (插入) 和对现有记录的更新使用`键/值`对来表示.
  大多数现代存储系统不显式地删除页上的数据.
* 相反, 它们使用删除标记 (`deletion marker`, 也称为墓碑 (`tombstone`)),
  其中包含此删除动作的元数据, 如键和时间戳.
  - 在垃圾收集过程中, 这些被更新或被删除标记遮盖 (`shadowed`)
    过的记录所占用的空间会被数据库回收.
  - 该过程会读取页, 然后将活动 (即未被遮盖)
    的记录写入新位置, 并丢弃被遮盖的记录.

* 主 (数据) 文件上的索引称为主索引. 但是, 在大多数情况下,
  我们还可以假设主索引是在主键或作为主键的一组键之上构建的.
  - 所有其他索引都称为二级索引 (`secondary index`).
* 二级索引可以直接指向数据记录, 也可以简单地存储它的主键.
  指向数据记录的指针可以保存堆文件或索引组织表中的偏移量.

* 如果数据记录的顺序遵循搜索键顺序, 则这种索引称为聚簇索引
  (`clustered/clustering index`).
  聚簇索引中的数据记录通常与索引存储于同一文件,
  有时也存放在单独的聚簇文件中, 而这些文件均保留了键的顺序.
* 如果数据存储在单独的文件中, 且其顺序不遵循键顺序,
  则索引称为非聚簇索引 (`nonclustered/unclustered index`).

* 索引组织表以索引的顺序保存数据, 因此按定义一定是聚簇的.
  主索引通常是聚簇的, 而根据定义, 二级索引一定不是聚簇的,
  因为它们是用于加速主键以外的键的访问的.
  - 聚簇索引既可以是索引组织的,
    也可以具有单独的索引和数据文件.

* 如果工作负载主要由读操作组成, 那么更新几个索引可能没有什么问题,
  但是对于具有多个索引, 以写为主的工作负载, 这种方法便不能很好地工作了.
* 为了减少指针更新的成本, 一些数据库的具体实现是使用主键进行间接操作,
  而不是直接使用数据偏移量.

* 通常, `LSM`树和`B`树之间的区别便是数据是不可变的还是原地更新的,
  但是也存在受`B`树启发但不可变的数据结构.

## B树基础知识

* 在深入研究`B`树之前, 让我们先讨论一下为什么应该考虑替代传统搜索树
  (例如, 二分搜索树, `2-3`树和`AVL`树).

* 一棵**二分搜索树**中只能有一个根节点.
* 每个节点将搜索空间分为左子树和右子树:
  - 一个节点的键大于其左子树中存储的任何键,
  - 且小于其右子树中存储的任何键.
* 插入操作并不会遵循任何特定模式, 元素插入可能导致树不平衡的情况
  (即它的一个分支比另一个分支长).
  - 最差的情况得到一棵病态树, 更像一个链表,
    此时我们得到的不是期望的对数时间复杂度,
    而是线性时间复杂度.

* 平衡树指的是高度为
  $$ \log_2 N $$
  的树 (其中 `N` 是树中数据项的总数),
  并且两个子树之间的高度差不大于 `1`.

* 树的平衡是通过以最小化树高并将每一边的节点数保持在界限内的方式重新组织节点来完成的.
* 保持树的平衡的方法之一是在添加或删除节点后执行旋转:
  - 如果插入操作使分支不平衡
    (分支中的两个连续节点只有一个子节点),
    则可以围绕中间节点旋转树.

* 由于扇出较低 (扇出是指每个节点允许拥有的最大子节点数),
  我们必须相当频繁地执行平衡操作, 重新定位节点并更新指针.
  - 维护成本的增加使得二分搜索树作为存储在磁盘上的数据结构变得不切实际.

* 适合磁盘实现的树必须具有以下属性:
  - 高扇出, 以改善邻近键的数据局部性.
  - 低高度, 以减少遍历期间的寻道次数.

* 典型的 `SSD` 由记忆单元构成, 这些单元连接成串
  (每个串通常为 `32` 到 `64` 个单元),
  串被组合成阵列, 阵列被组合成页, 页被组合成块.
* 可写 (可编程) 或可读的最小单元是页. 但是, 我们只能对空的记忆单元进行更改
  (即, 对写入之前已擦除的单元进行更改).
* 最小的擦除实体不是页, 而是保存多个页的块, 这就是为什么它通常被称为擦除块.
  空块中的页必须按顺序写入.
* 每当我们从块设备读取单个字时, 包含它的整个块将会被读取.
  这是我们不能忽视的一个限制, 在处理基于磁盘存储的数据结构时应该始终考虑到这一点.

* 每当我们从块设备读取单个字时, 包含它的整个块将会被读取.
  这是我们不能忽视的一个限制,
  在处理基于磁盘存储的数据结构时应该始终考虑到这一点.

---

* `B`树是有序的: `B`树节点内的键按顺序存储. 因此,
  我们可以使用像二分搜索这样的算法来定位搜索到的键.
  这也意味着`B`树中的查找具有对数复杂度.
* 使用`B`树, 我们可以有效地执行单点查询和范围查询.
  - 在大多数查询语言中, 我们通过谓词相等
    `(=)` 表示单点查询来定位单个项,
  - 而通过谓词比较 `(<, >, ≤ 和 ≥)`
    表示范围查询来按顺序查询多个数据项.

* `B`树由多个节点组成. 每个节点最多可容纳`N`个键和`N+1`个指向子节点的指针.
  这些节点在逻辑上分为三类.
  - **根节点**: 根节点没有父节点, 是树的顶端.
  - **叶节点**: 叶节点是没有子节点的底层节点.
  - **内部节点**: 连接根节点和叶节点的其他节点,
    `B`树通常包含多层的内部节点.

* 由于`B`树是一种页组织技术 (即用于组织和导航固定大小的页的技术),
  所以节点和页这两个术语在描述中可相互替换.
* 节点容量与其实际持有的键的个数之间的关系称为占用率.
* `B`树的特征在于其扇出 (`fanout`): 存储在每个节点中的键的个数.
  - 为保持树的平衡需要做出一些结构上的更改,
    而更高的扇出则有助于均摊这些更改的所带来的开销.
* 同时, 通过在单个块或多个连续块中存储指向子节点的键和指针,
  可以减少寻道的次数.

* `B`树允许在根节点, 内部节点和叶节点当中的任意层上储存值.
  而`B+`树则仅在叶节点中存储值, 其内部节点仅存储分隔键,
  用于指引搜索算法去找到叶节点上的关联值.

* `B+`树广为人知, 因此我们像其他文献中一样称之为`B`树.

* 存储在`B`树节点中的键称为索引条目 (`index entry`),
  分隔键 (`separator key`) 或分隔符单元格 (`divider cell`).
* 它将树分割成子树 (也称为分支或子范围), 其持有包含对应键的范围.
  键存储时已经排好序, 以便使用二分搜索.
* 查找算法通过定位一个键并跟随相应的指针从较高的层次移动到较低的层次来找到一个子树.

* 使`B`树与众不同的是, 它不是自上而下构建的 (像二分搜索树那样),
  而是采用相反的构建方式 -- __自下而上__.
  - 随着叶节点数量的增加, 内部节点的数量和树的高度也将增加.
* 由于`B`树在节点内部为将来的插入和更新保留了额外的空间,
  所以树的存储占用率可以低至`50%`, 但通常这个数值要高得多.
  - 较高的占用率不会对`B`树的性能产生负面影响.

* 更准确地说, 如果以下条件成立, 则需要分裂节点:
  - 对于叶节点: 如果节点最多可以容纳 `N` 个键值对,
    且再插入一个键值对将使其超过其最大容量 `N`.
  - 对于非叶节点: 如果节点最多可以容纳 `N+1` 个指针,
    且再插入一个指针将使其超过其最大容量 `N+1`.
* 分裂是通过分配新节点,
  将一半元素从原分裂节点传输给它并添加它的第一个键和指向父节点的指针来完成的.
  - 在这种情况下, 我们说这个键被提升 (`promote`) 了.
    执行分裂处的数组下标称为分裂点 (也称为中点).
  - 分裂点之后的所有元素 (在非叶节点分裂的情况下, 包括分裂点)
    都被传输到新创建的兄弟节点, 其余元素保留在分裂节点中.
  - 如果父节点已满, 即没有容纳被提升的键和指向新创建节点的指针的空间时,
    也必须分裂父节点. 此操作可能会一直递归传播到根节点.

* 分裂完成后有两个节点, 我们必须选择正确的节点才能完成插入.
  为此, 我们可以使用分隔键不变量.
  - 如果插入的键小于要提升的键, 则最后插入到原分裂节点.
  - 否则, 我们将要插入的键放入新创建的节点.
* 总之, 节点分裂分为四个步骤:
  1. 分配一个新节点.
  2. 将一半元素从分裂节点复制到新节点.
  3. 将新元素放入相应节点.
  4. 在分裂节点的父节点处, 添加一个分隔键和指向新节点的指针.

* 更准确地说, 如果满足以下条件, 则合并两个节点:
  - 对于叶节点: 如果一个节点可以容纳最多 `N` 个键值对,
    并且两个相邻节点中的键值对的数目加起来小于或等于 `N`.
  - 对于非叶节点: 如果一个节点可以容纳最多 `N+1` 个指针,
    并且两个相邻节点中指针的数量加起来小于或等于 `N+1`.

* 总之, 假设元素已经被删除, 节点合并分为三步:
  1. 从右节点复制所有元素到左节点.
  2. 从父节点删除右节点指针 (如果是非叶子节点合并, 则将此指针进行降级).
  3. 删除右节点.
* 为了减少分裂和合并的次数, `B` 树经常采用的技术之一是再平衡.

## 文件格式

* 磁盘数据结构中指针管理的语义与内存中的有所不同,
  你可以将磁盘上的`B`树看作一种页管理机制:
  - 算法需要组合页并在页中移动.
  - 需要计算页和指向它们的指针并将它们放置在相应的位置.

* 大多数的数值类型都用固定大小的值来表示.
  当处理多字节数值时, 务必在编解码时使用相同的**字节序**
  (`byte-order` 或 `endianness`),
  **字节序**决定了一组字节的先后顺序.
  - **大端**: 从最高有效字节 (MSB) 开始,
    从高位到低位依次排列.
    换句话说, 最高有效字节具有最低的地址.
  - **小端**: 从最低有效字节 (LSB) 开始,
    从低位到高位依次排列.

* 通常, 在设计一种文件格式时, 首先要确定寻址方式:
  是否要将文件拆分为相同大小的页,
  哪些页由单个块或多个连续块所组成.
* 大多数原地更新的存储结构都使用相同大小的页,
  从而大大简化了读取和写入访问.
* 仅追加 (append-only) 的存储结构通常也按页写入数据:
  记录被一条一条地追加上去, 一旦内存中该页写满了,
  就将其刷写到磁盘上.
* 文件通常以定长的头部 (header) 开始,
  可能以一个定长的尾部 (trailer) 结束.
  - 尾部包含需要被快速访问的辅助信息或解析文件其余部分所必要的信息.

* 数据库将数据记录存储在数据文件和索引文件中.
  这些文件被划分为固定大小的单元, 称为页.
  - 页大小通常是文件系统块的整数倍, 一般是 `4~16KB`.

* 从数据结构的角度来看, 在`B`树中,
  我们区分了叶节点 (包含键和数据记录对)
  与非叶节点 (包含键和指向其他节点的指针).
  - 每个`B`树节点占据一个页或多个链接在一起的页,
    因此在讨论`B`树时, "节点"和"页" (甚至"块")
    这几个术语经常可以互换使用.

* 空间回收可以通过简单地重写页并移动记录来完成,
  但是需要保证记录的偏移量不变, 因为页外的指针会用到这些偏移量.
  在做到这一点的同时, 我们希望尽可能减少空间浪费.
* 总之, 我们需要一种页格式, 它允许我们:
  - 以最小的开销存储变长记录
  - 回收已删除记录所占用的空间
  - 引用页中的记录, 无论这些记录具体在什么位置

* 我们将页组织成一个槽或单元格 (cell) 的集合,
  并将指针和单元格分别存放在页两侧的独立内存区域中.
  - 若想保持记录原来的顺序, 我们只需要重新组织指向单元格的指针;
    若要删除一条记录, 我们只需将记录的指针置为空或删除指针即可.
* 分槽页具有一个固定大小的头部, 其中包含关于页和单元格的重要信息.
  单元格的大小可能各不相同, 并且可以容纳任意数据:
  - 键, 指针, 数据记录等.

* **最小开销**: 分槽页唯一的额外开销是一个指针数组,
  用于保存记录实际所在位置的偏移量.
* **空间回收**: 通过对页进行碎片整理和重写, 就可以回收空间.
* **动态布局**: 从页外部, 只能通过槽`ID`来引用槽,
  而确切的位置是由页内部决定的.

* 我们假定单个页内所有单元格是统一的
  - 例如, 要么全是键单元格, 要么全是键值单元格;
  - 类似地, 要么全都包含定长数据, 要么全都包含变长数据,
    但不能是二者的混合.
* 这样一来, 单元格的元数据只要在每个页上保存一份即可,
  而不用让每个单元格都保存一份.

* 从页删除一条记录不用删除实际的单元格,
  也不用移动其他单元格以重用这些释放的空间.
  相反, 可以将这个单元格标记为已删除,
  并根据被释放内存的大小以及指针更新内存中的可用列表.
* 可用列表保存了可用段的偏移量及其大小. 每当插入新单元格时,
  我们首先检查可用性列表, 看看是否有能放得下的段.

* 至于具体要使用哪一个空闲块, 是通过以下策略计算的:
  - **首次适配优先**
  - 这种方法可能会造成较大的额外开销,
    因为当我们把数据填进第一个合适的段之后,
    剩余的空间可能不够放下其他的单元格,
    因而被浪费掉了.
  - **最佳适配优先**
  - 在最佳适配中, 我们尝试寻找一个段,
    使得插入之后段内剩余的空间最小.

* 如果找不到足够长的连续字节来存放新的单元格,
  但有足够多的碎片字节可用, 我们会读出所有存活的单元格再重新写入,
  即对页进行碎片整理, 以回收空间来留给新的写入.
* 如果在碎片整理之后依然没有足够的可用空间, 我们就要创建一个溢出页.

* 非加密哈希和 `CRC` 不应当用于验证数据是否已被篡改.
  对于这类场景, 请务必使用专为安全性设计的强加密哈希.
* `CRC` 的主要目标是确保数据没有非人为的, 意外的变化,
  而非用于抵御攻击或人为的修改.

* 由于计算整个文件的校验和通常是不切实际的,
  而且不太可能每次访问文件都读取全部内容,
  所以校验和通常是针对每个页计算的, 并保存在页头部.
* 这样一来, 校验和可以更健壮 (因为仅针对一小部分数据),
  而且就算单个页发生损坏, 我们也不用丢弃整个文件.

## B树的实现

* 存储同级指针的缺点之一是在分裂和合并期间必须更新它们.
  由于更新必须在同级节点中进行,
  而不是在`分裂/合并`节点中进行,
  所以可能需要额外的锁.

* `B`树分隔键有严格的不变式: 它用于将树拆分为子树并对这些子树进行遍历,
  因此指向子页的指针总是比指向键的指针多一个.
* 在许多实现中: 每个分隔键都有一个子指针, 而最后一个指针则单独存储,
  因为它并没有与任何键相匹配.
* 如果最右侧的子指针被拆分, 并且新的单元格被追加到其父节点上,
  则必须重新分配最右子指针.

* `B`树算法规定每个节点持有特定数量的元素. 由于某些值具有不同的大小,
  所以根据`B`树算法, 可能会出现这样一种情况:
  - 节点尚未满, 但保存该节点固定大小的页上已经没有更多的可用空间了.
  - 调整页大小需要将已经写入的数据复制到新区域, 这通常是不切实际的.
    但我们仍然需要找到一种方法来增加或扩展页大小.
* 为了实现变长节点而无须将数据复制到新的连续区域,
  我们可以从多个链接起来的页中构建节点.
  - 例如, 默认页大小为`4K`, 而在插入几个值之后, 其数据大小增长到`4K`以上.
  - 此时我们并不使用任意大小的页, 而是允许节点以`4K`为增量进行增长,
    因此我们可以分配一个`4K`的扩展页, 并从原始页链接到它.
  - 这些链接起来的页被称为溢出页 (overflow page),
    我们将原始页称为主页 (primary page).

* 二分搜索算法接收一个包含已排序元素的数组和一个搜索键, 并返回一个数字.
  如果返回的数字是整数, 则我们找到了要搜索的键,
  并且数字指定了它在输入数组中的位置.
  而返回值为负数则表示搜索到的键不存在于输入数组中, 并给出了一个插入点.

* 如果节点被分裂或合并, 则可以使用导航信息来为上拉到父节点的键查找插入点,
  并在必要时沿着树向上走, 以将结构更改传播到更高层的节点.
  - 这个栈一般在内存中维护.

* 一些`B`树的实现方案试图推迟分裂和合并操作, 以便通过在层内再平衡各元素来平摊代价,
  或将元素从占用较多的节点转移到占用较少的节点中, 通过这一方式尽可能推迟分裂或合并.
  - 虽然维护代价可能更高一些, 但这有助于提高节点利用率以及减少树的层数.

* 许多数据库系统使用单调自增的数值作为主索引的键. 这为优化创造了机会,
  因为所有的插入都发生在索引的末尾 (在最右边的叶子中),
  所以大多数分裂发生在每层的最右节点上.
* 此外, 由于键是单调递增的, 所以考虑到追加相对更新和删除的比例很低,
  相对于键随机排列的情况而言, 非叶页上的碎片化程度更低.

* 我们可以在不同的粒度级别上进行压缩. 尽管压缩整个文件可以产生更好的压缩率,
  但由于整个文件必须在更新时被重新压缩, 所以它的应用有限.
  - 更细粒度的压缩通常更适合较大的数据集.

## 事务处理与恢复

* 页缓存充当了持久性存储 (磁盘) 和存储引擎其余部分之间的中介.
  它将状态更改暂存在内存中, 同时也用于缓存那些尚未与持久性存储同步的页.
  一切数据库状态的更改都首先被应用在缓存的页上.
* 日志管理器记录了已应用在缓存页上的操作 (日志条目),
  这些操作尚未与持久性存储同步, 而日志可以确保这些操作不会在崩溃时丢失.
  - 换句话说, 在数据库启动期间, 我们利用日志来重新应用这些操作并重建缓存状态.
    日志条目也可以用来撤销已中止的事务所做的更改.

* 假设没有其他进程修改磁盘上的数据, 则内存中的缓存页可以被重用.
  这种做法有时也称为虚拟磁盘. 仅当内存中没有页副本可用时,
  对虚拟磁盘的读取才会访问物理存储.
* 更多时候, 我们称上述机制为页缓存或缓冲池.
  页缓存负责缓存从磁盘读取的页. 如果数据库崩溃或意外关闭,
  则缓存的内容也将丢失.

* 将未缓存的页从磁盘加载进来的过程称为换入 (page in).
  缓存的页一旦被更改过就成了脏页,
  直到这些更改被刷写 (flush) 到磁盘上.

* 页缓存的主要功能可以总结为以下几点:
  - 在内存中保留被缓存的页的内容.
  - 把对磁盘页的修改缓冲起来, 并且修改的是缓存的版本.
  - 当被请求的页不在内存中且可用空间足够时,
    页缓存会将其换入并返回缓存的版本.
  - 如果请求的页在缓存中, 则直接返回缓存的版本.
  - 如果可用空间不足以放下新的页, 则某些其他页会被换出,
    被换出的页的内容会被刷写回磁盘.

* 如果某个页被修改了 (例如追加了一个新的单元格),
  则该页被标记为脏页. 页上的脏标志位表示其内容与磁盘不同步,
  必须将其刷写到磁盘上才能保证持久性.

* 另一个要记住的重要属性是持久性: 如果数据库崩溃,
  则所有未刷写的数据都会丢失. 为了确保所有更改都被持久化,
  检查点进程会协调刷写进程.
* 检查点进程控制预写日志 (WAL) 和页缓存, 并确保两者协同工作.
  只有当缓存页完成刷写之后, 相关操作的日志记录才能从 `WAL` 中丢弃.
  在上述过程完成后才能将脏页换出缓存.

* **LRU** 也按插入顺序维护一个换出候选队列, 但是当我们重复访问某个页时,
  `LRU` 会将其放回队列尾部, 就像首次换入时那样. 然而,
  并发环境下每次访问都要更新引用和重新链接节点可能代价较高.
* 在某些情况下, 效率可能比精度更重要. **CLOCK**
  算法变体常常被用作 `LRU` 的一种更加紧凑, 更加缓存友好且并发性更好的替代品.

* **CLOCK-sweep** 算法将页的引用和与之关联的访问位保存在环形缓冲区中.
  一些变体使用计数器而不是比特位来描述频率.
* 每当访问某个页面时, 都将它的访问位设置为`1`. `CLOCK`
  算法的工作原理是循环检查环形缓冲区上的访问位:
  - 如果访问位为`1`且该页未被引用, 则将其置为`0`并检查下一页.
  - 如果访问位已经为`0`, 则将该页作为一个要换出的候选,
    并安排在后续将其换出.
  - 如果页当前正被引用, 则它的访问位保持不变.
    算法假定被访问页的访问位不可为`0`,
    因此不会被换出缓存, 这使得被引用的页更不可能被置换.

* 使用环状缓冲区的一个优点是:
  - 时钟指针和缓冲区内容都可以用`比较-置换` (**CAS**)
    原子操作来修改, 无须额外的加锁机制.

---

* **预写日志** (Write-Ahead Log, **WAL**, 也称为提交日志)
  是一种仅追加的辅助磁盘数据结构, 用于崩溃和事务的恢复.
  - 页缓存允许我们在内存中缓冲对页面内容的更改,
  - 而在将缓存的内容刷写回磁盘之前,
    `WAL`是保留操作历史的唯一磁盘副本.

* 预写日志的主要功能可以概括为:
  - 在允许页缓存将页上的修改缓存起来的同时,
    保证数据库系统仍然具有持久性语义.
  - 在那些受操作影响的缓存页被同步到磁盘上之前,
    将所有操作持久化到磁盘上.
  - 每个修改数据库状态的操作必须先写日志到磁盘上,
    然后才能修改相关页的内容.
  - 当发生崩溃时, 使系统可以从操作日志中重建内存中丢失的更改.
* 除此以外, 预写日志在事务处理中也起着重要的作用.
  它确保将数据存储到持久性存储中, 即使发生崩溃也依然可用:
  - 通过重放日志便可以恢复未提交的数据,
    这样数据库就可以完全恢复到崩溃前的状态.

* 由于 `WAL` 是一个不可变的, 仅追加的数据结构,
  所以读取方可以安全地访问写入边界之前的内容,
  同时写入方可以继续将数据追加到日志尾部.
* `WAL` 由日志记录组成, 每条记录都有一个唯一的,
  单调递增的日志序列号 (LSN). 通常, `LSN`
  由一个内部的计数器或时间戳表示.
* 由于日志记录不一定占据整个磁盘块, 所以其内容会被缓存在日志缓冲区中,
  并在强制刷盘 (force) 操作时被刷写到磁盘上.
  强制刷盘操作发生在日志缓冲区被填满时,
  也可能被来自事务管理器或页缓存的请求所触发.
  各日志记录必须以 `LSN` 的顺序刷写到磁盘上.
* 除了单独的操作记录外, `WAL` 还会保存事务完成的记录.
  只有当事务提交记录完成刷盘之后, 才能将该事务视为已提交.
* 系统在回滚或恢复期间也有可能发生崩溃, 为了确保这种情况下系统能继续正常工作,
  某些系统会在撤销操作时记录补偿日志记录 (`CLR`) 并将其存储在日志中.

* 物理日志记录了前像和后像, 受操作影响的整个页都要被记录下来.
  逻辑日志记录了要对页应用哪些操作, 例如"向键`Y`插入数据记录`X`",
  以及相应的撤销操作, 例如"删除与`Y`关联的值".
* 在实践中, 许多数据库结合使用上述两种方法:
  - 使用逻辑日志记录执行撤销操作 (以提升并发),
  - 使用物理日志记录执行重做操作 (以缩短恢复时间).

* `ARIES` 是一种 `steal/no-force` 的恢复算法.
  它使用物理重做日志来提高恢复期间的性能 (因为能更快地应用更改),
  并使用逻辑撤销日志来提高正常操作期间的并发
  (因为逻辑撤销操作可以独立地应用到页中).
  - 它使用 `WAL` 来实现在恢复时重放历史, 从而完整地重建数据库状态
    (未提交事务的修改已被撤销), 并在撤销期间构建补偿日志记录.

---

* 当数据库崩溃后重启时, 恢复过程分为三个阶段:
  - 分析阶段识别出页缓存中的脏页以及崩溃时正在进行的事务.
    脏页的信息用于标识重做阶段的起点.
    进行中事务的清单用于在撤销阶段中回滚未完成的事务.
  - 重做阶段重放历史记录直到崩溃点, 并将数据库恢复到先前的状态.
    此阶段会处理未完成的事务以及那些已提交但尚未将修改刷写到持久化存储的事务.
  - 撤销阶段回滚所有未完成的事务, 并将数据库还原到最后的一致状态.
    所有操作均按反向时间顺序回滚. 为了防止数据库在恢复过程中再次崩溃,
    撤销事务所做的操作也会被记录到日志中, 以避免重复应用.

* `ARIES` 使用 `LSN` 来识别日志记录,
  通过脏页表来追踪运行中事务修改过的页,
  并使用物理重做, 逻辑撤销和模糊检查点.

---

* 事务管理器和锁管理器协同工作以处理并发控制.
  并发控制是一组用于处理并发事务间交互的技术.
  这些技术可以大致分为以下几类:
  - **乐观并发控制**
  - 乐观并发控制 (`OCC`) 允许多个事务执行并发的读取和写入操作,
    最后确定其执行结果能否被串行化 (serializable).
  - 换句话说, 事务不会彼此阻塞, 而是保留其操作历史,
    并在提交前检查这些历史操作是否存在冲突的可能.
    如果会产生冲突, 则中止其中某一个冲突的事务.
  - **多版本并发控制**
  - 多版本并发控制 (`MVCC`) 允许一条记录同时存在多个时间戳的版本,
    通过这种方式保证事务读到的是数据库过去某个时刻的一致视图.
  - `MVCC` 可以使用验证技术来实现, 即只允许多个更新或事务提交中的某一个获胜,
    也可以使用无锁技术 (例如时间戳排序) 或基于锁的技术 (例如两阶段锁).
  - **悲观并发控制**
  - 悲观 (也称为保守) 并发控制 (`PCC`) 既有基于锁的实现,
    也有不加锁的实现, 它们的主要区别在于如何管理和授权对共享资源的访问.
  - 基于锁的实现要求事务维护数据库记录上的锁,
    以防止其他事务修改被加锁的记录或访问当前事务正在修改的记录,
    直到锁被释放为止.
  - 不加锁的实现根据未完成事务的调度, 维护读取与写入的操作列表以限制事务的执行.
    `悲观的调度可能导致死锁`: 多个事务需要互相等待对方释放锁才能继续执行.

* **事务**包括对数据库状态的一系列读取和写入操作以及业务逻辑 (应用于读取内容上的转换).
* **调度** (schedule) 是指数据库视角上执行一组事务所需的操作列表
  (即仅包含与数据库状态交互的操作, 例如读, 写, 提交和中止),
  因为我们假定其他所有的操作都没有副作用 (换句话说, 不影响数据库状态).
* 如果一个调度中包含其中每个事务的所有操作, 则称它是**完整的**.
* 一个正确的调度在逻辑上等效于原始操作列表, 但是可以并行执行其中某些部分,
  也可以出于优化目的对其进行重新排列, 只要不违反
  `ACID` 性质并保证各事务的结果正确即可.

* 如果一个调度中的事务完全独立且无交错地执行, 则我们称它为串行的调度:
  - 每个事务在下一个事务开始之前已完全执行.
    相比于多个事务的各种交错执行, 串行执行很容易进行论证.
  - 但是, 总是一个接一个地执行事务将会大大限制系统吞吐量并损害性能.
* 我们需要找到一种方法, 它能够并发执行事务操作, 同时保持串行调度的正确性和简单性.
  **可串行化**的调度可以满足这一要求.
  - 如果一个调度等效于同一组事务的某一完整串行调度,
    则该调度是可串行化的.
  - 换句话说, 它产生的结果与我们以某种顺序一个接一个地执行一组事务的结果相同.

---

* 在执行并发事务期间可能发生的读异常:
  - **脏读**
  - **不可重复读**
  - **幻读**

* **脏读** (dirty read) 是指一个事务能读到其他事务未提交的更改.
* **不可重复读** (nonrepeatable read),
  是指同一事务两次查询同一行却得到不同的结果.
  - 有时也称为模糊读 (fuzzy read)
* 如果我们在事务执行过程中使用范围读取
  (即不是读取单个数据记录, 而是读取一个范围内的记录), 则可能会看到幻记录.
  - **幻读**是指事务两次查询同样的行集合却得到不同的结果.
  - 它与`不可重复读`类似, 但仅适用于**范围查询**.

* 同样地, 也存在具有类似语义的写异常:
  - **丢失更新**
  - **脏写**
  - **写偏斜**

* **丢失更新** (lost update) 发生在事务 `T1` 和 `T2` 同时尝试更新 `V` 的值时.
* **脏写** (dirty write) 指的是某个事务拿到了一个未提交的值 (即脏读),
  对其进行修改并保存.
  - 换句话说, 事务结果来自从未提交过的值.
* **写偏斜** (write skew) 是指各个单独的事务都遵守要求的约束,
  但它们的组合却违反了这些约束.

* 最低 (换句话说, 最弱) 的隔离级别是**读未提交** (read uncommitted).
  - 在此隔离级别下, 事务系统允许一个事务观察到其他并发事务的未提交更改.
  - 也就是说, **脏读**是允许的.
* 如果两次读取之间存在已提交的修改, 则同一事务中的两次查询会得到不同的结果.
  换句话说, 脏读是不允许的, 但**幻读**和**不可重复读**是允许的.
  该隔离级别称为**读已提交** (read commited).
* 如果我们进一步禁止不可重复读, 则会得到**可重复读**
  (repeatable read) 的隔离级别.
* 最强的隔离级别是**可串行化** (serializability).

* 不同于**可线性化**, **可串行化**是指按任意顺序执行多个操作,
  它并不暗示或强制某一特定的事务执行顺序.
  - `ACID` 中的**隔离性**意味着**可串行化**.
* 然而, 实现可串行化需要协调, 换句话说,
  并发执行的事务必须进行协调以确保遵守约束,
  并在发生冲突时强加某一串行顺序.

---

* **乐观并发控制**假设事务冲突很少发生, 并且不同于加锁阻塞事务执行的方式,
  我们可以通过在事务提交前验证事务来防止并发事务间发生读或写冲突, 确保可串行化.
  一般来说, 事务执行分为三个阶段:
  - **读阶段**
  - 事务在其私有上下文中执行各步骤, 此时一切更改都对其他事务不可见.
    在该步骤完成后, 我们就知道了事务的所有依赖条件 (读集合),
    以及事务会产生的所有副作用 (写集合).
  - **验证阶段**
  - 检查并发事务的读集合和写集合, 查看其操作之间是否存在可能违反可序列化性质的冲突.
    如果事务读取的某些数据现在已过期,
    或者事务的写入将覆盖在其读阶段提交的事务所写入的某些值,
    则清除事务的私有上下文, 重启读阶段.
  - 换句话说, 验证阶段确定提交事务是否遵守 `ACID` 性质.
  - **写阶段**
  - 如果验证阶段未发现任何冲突, 则事务可以将其写集合从私有上下文提交到数据库状态中.

* 验证有两种实现方式: 检查与已提交事务的冲突 (**后向式**),
  以及检查与当前处于验证阶段的事务的冲突 (**前向式**).
  - 不同事务的验证阶段和写阶段应当原子地完成.
    在验证其他事务时, 不允许提交任何事务.
  - 由于验证阶段和写阶段通常比读阶段短, 因此这是可以接受的妥协.
* **后向式**并发控制确保任意一对事务 `T1` 和 `T2` 都具有以下性质:
  - 如果 `T1` 在 `T2` 的读阶段开始之前提交, 则 `T2` 可以提交.
  - 如果 `T1` 在 `T2` 的写阶段之前提交, 则 `T1` 的写集合与 `T2` 的读集合不相交.
    换句话说, `T1` 不能写入任何 `T2` 看到的值.
  - 如果 `T1` 的读阶段在 `T2` 的读阶段之前完成
    (注: 但不满足上一条的情况, 即在 `T2` 进行写阶段时 `T1` 还未提交.),
    则 `T2` 的写集合与 `T1` 的读或写集合均不相交.
    换句话说, 两个事务操作独立的数据集合, 因此两者都可以提交.

---

* **多版本并发控制**是数据库中实现事务一致性的一种方法, 它允许记录存在多个版本,
  并使用单调递增的事务 `ID` 或时间戳.
  - 这使得读写操作在存储层面上只需要最小限度的协调,
    因为读操作可以继续访问旧的值, 直到新的值被提交.
* `MVCC` 区分已提交和未提交的版本, 对应于已提交和未提交事务的值的版本.
  最后提交的值的版本也就是值的当前版本. 一般在 `MVCC` 中,
  事务管理器的目标是确保任一时刻最多只有一个未提交的值版本.
* 根据数据库实现的隔离级别, 读操作也许能访问或不能访问未提交的值.
  多版本并发可以用加锁, 调度和冲突解决技术 (例如两阶段锁) 来实现,
  也可以用时间戳排序来实现. 实现快照隔离是 `MVCC` 的一大使用场景.

* **悲观并发控制**方案比乐观方案更保守.
  这种方案在事务运行时确定其间的冲突并阻塞或中止执行.
* 时间戳排序是最简单的悲观 (无锁) 并发控制方案之一,
  其中每个事务都有一个时间戳,
  事务操作能否执行取决于是否已经提交过时间戳更晚的事务.
* 为了实现这一点, 事务管理器需要维护每个值的
  `max_read_timestamp` 和 `max_write_timestamp`,
  它们分别描述了并发事务执行的读和写操作.
* 如果读操作的时间戳小于读到的值的 `max_write_timestamp`,
  则会导致当前事务中止. 因为已经有一个较新的值存在,
  若允许该操作则会违反事务顺序.
* 类似地, 时间戳小于 `max_read_timestamp` 的写操作将与最近的读操作冲突.
  但是这种情况是被允许的, 因为我们可以安全地忽略过期的写入值.
* 这个猜想通常被称为托马斯写规则. 每当进行读或写操作时,
  相应的最大时间戳都会被更新. 中止的事务会以新的时间戳重新开始,
  否则新的事务必定会被再次终止.

---

* 基于**锁**的并发控制方案是**悲观**并发控制的一种形式,
  它对数据库对象显式地加锁, 而不是用时间戳排序之类的协议来制定操作的顺序.
  - 使用锁的缺点包括锁竞争和扩展性问题.
* 使用最广泛的基于锁的技术之一是**两阶段锁** (`2PL`),
  它将锁管理分为两个阶段:
  - **增长**阶段 (也称为扩张阶段), 此阶段将获取事务所需的所有锁, 并且不释放任何锁.
  - **收缩**阶段, 此阶段释放增长阶段获得的所有锁.
* 由上述两个定义可以推断出一条规则:
  - 事务一旦释放了哪怕一个锁, 就不能再获取任何锁.
  - 值得注意的是, `2PL` 并不阻止事务在这两个阶段中执行事务步骤,
    但是某些 `2PL` 变体 (例如 `保守 2PL`) 确实增加了这些限制.

* 尽管名称很相似, 但**两阶段锁**和**两阶段提交**是两个完全不同的概念.
  - **两阶段提交**是一种用于分布式多分区事务的协议,
    而两阶段锁是一种并发控制机制, 常用于实现可串行化.

---

* **死锁**
* 在锁协议中, 事务尝试获取数据库对象上的锁, 如果无法立刻获得锁,
  则事务必须等待直到锁被释放. 可能会发生这样的情况:
  - 两个事务都在尝试获取所需的锁以继续执行,
    而最后都在等待对方释放其持有的某个锁.
* 最简单的处理死锁的方法是引入超时机制并中止长时间运行的事务,
  假定它们可能处于死锁状态.
* 另一种策略, 即 `保守 2PL`, 要求事务在执行任何操作之前先获取所有锁,
  如果做不到就中止事务. 但是, 这些方法极大地限制了系统的并发,
  因此数据库系统大多使用**事务管理器**来检测或避免 (或者说防止) 死锁.

* 为了避免死锁, 限制锁的获取以确保不会产生死锁,
  事务管理器可以用事务时间戳来确定事务优先级.
  - 较低的时间戳通常意味着较高的优先级, 反之亦然.
* 如果事务 `T1` 尝试获取 `T2` 当前持有的锁,
  并且 `T1` 具有更高的优先级 (它在 `T2` 前开始),
  那么我们可以使用以下限制之一来避免死锁:
  - **等待-死亡** (wait-die)
  - 允许 `T1` 阻塞以等待锁. 否则 (注: 如果 `T2` 在 `T1` 之前开始.),
    `T1` 将中止并重新启动. 换句话说, 事务只能被时间戳更高的事务阻塞.
  - **伤害-等待** (wound-wait)
  - `T2` 中止并重新启动 (`T1` 伤害 `T2`). 否则 (如果 `T2` 在 `T1` 之前开始),
    则允许 `T1` 等待. 换句话说, 事务只能被时间戳更低的事务阻塞.
  - 事务处理需要一个调度器来处理死锁. 同时,
    **闩锁**依靠程序员来保证不会发生死锁, 而不依赖于死锁避免机制.

* **锁**
* 在事务处理过程中, 保护逻辑和物理数据完整性的机制有所不同.
  负责逻辑和物理完整性的两个概念分别是**锁** (lock) 和**闩锁** (latch).
* 其在命名上有点令人迷惑, 因为这里说的闩锁在系统编程中通常称为锁,
  但在本节中我们将阐明它们的区别和含义.
* 锁用于隔离和调度重叠的事务, 管理数据库内容 (而非内部存储结构),
  并且锁是在键上获取的. 锁可以保护某个特定的键 (无论该键存不存在)
  或一个范围内的键.
* 锁通常在树实现之外进行存储和管理, 它表示一个较高层级的概念,
  由数据库的锁管理器管理.
* **锁比闩锁更重量级**, 且在事务执行期间一直持有.
* **闩锁**
* 另一方面, 闩锁用于保护物理表示: 在插入, 更新和删除操作期间,
  叶子页的内容会被修改; 在叶子页发生下溢或上溢时,
  页的分裂与合并向上传播, 非叶子页的内容以及树结构也会被修改.
* 在这些操作期间, 闩锁保护了树的物理表示 (页内容及树结构),
  并且它是在页的级别上获取的. 在访问任何页前必须先加闩锁,
  以确保并发安全. **无锁并发控制技术也必须使用闩锁**.

---

* **读写锁**
* 最简单的闩锁实现将会授予请求线程排他性的读写访问权限.
  但是, 大多数时候, 我们不需要把所有进程相互隔离.
  - 例如, 并发地读取页不会有什么问题, 因此,
    我们只要确保并发的写操作不重叠, 以及读操作与写操作不重叠.
  - 为了达到这种粒度级别, 我们可以使用**读写锁** (又称为 **RW锁**).
* 读写锁允许多个读操作同时访问该对象, 而只有写操作 (通常相对较少)
  必须获得对该对象的排他性访问.
* 读写锁的兼容性表:
  - 只有读操作 (`读读`) 可以共享锁所有权,
    而读写操作的其他组合 (`读写`, `写写`) 都要获得排他性所有权.

## B树的变体

* **写时复制B树**, 其结构类似于`B`树, 但其中的节点是不可变的,
  并且不会原地更新. 相反, 页被复制, 更新并写入新的位置.
* **惰性B树**, 通过缓冲对节点的更新减少来自后续相同节点写操作的
  `I/O` 请求数量.
  - 我们还将介绍`双组件LSM树`, 它扩展了缓冲以实现完全不可变的`B`树.
* **FD树**则采用不同的缓冲方法, 有点类似于`LSM`树.
  - `FD`树在一个小型`B`树中缓冲更新. 一旦这个树被填满,
    它的内容就会被写入一个不可变的运行实例中.
  - 更新以级联的方式在不可变运行实例的层之间传播, 从较高的层传播到较低的层.
* **Bw树**将`B`树节点分成几个较小的部分, 以仅追加的方式进行写入.
  - 它通过批处理不同节点上发生的更新, 减少了少量但频繁的写操作的成本.
* **缓存无关B树** (cache-oblivious B-Tree)
  则允许以类似于内存中数据结构的构建方式处理磁盘上的数据结构.

---

* 有些数据库并不构建复杂的锁机制, 而是使用写时复制 (copy-on-write)
  技术来保证并发操作时的数据完整性. 在这种情况下, 每当页即将被修改时,
  就会先复制其内容, 然后修改复制的页而不是原来的页.
  - 这样便创建出了一个平行的树状层次结构.
* 旧版本的树对于与写入者并发的读取者而言仍然是可访问的,
  而访问修改后的页的写入者必须等待之前的写操作完成.
  在创建新的页层次结构之后, 指向最顶层页的指针将被自动更新.
* 由于必须复制整个页的内容, 所以这种方法的一个明显的缺点是它需要更多的空间
  (即使旧版本只会保留很短的时间: 在使用旧页的并发操作完成之后, 那些页可以被立即回收)
  和处理器时间.
  - 由于`B`树通常是较浅的, 所以这种方法的简洁性和优势往往仍然大于其缺点.
* 这种方法最大的优点是读取者不需要同步. 因为已写入的页是不可变的,
  可以在不需要额外加锁的情况下被访问. 因为写操作是针对复制页进行的,
  所以读取者不会阻塞写入者. 任何操作都不会得到一个处于不完整状态的页.
  即使是系统崩溃也不会使页处于损坏状态,
  因为只有当所有页的修改都完成时, 最顶端的指针才会切换.

---

* 让我们来看看如何使用缓冲实现**惰性B树**. 为此,
  我们可以在`B`树节点被换入后立即在内存中物化它们,
  并使用该结构存储更新, 直到我们准备好刷写它们.
* `MongoDB` 现在默认的存储引擎 `WiredTiger` 也使用了类似的方法.
  它的行存储`B`树的实现针对内存和磁盘页使用了不同的格式.
  - 在持久化内存页之前, 它们必须经过一个协调的过程.
* 在读取过程中会访问更新缓冲区: 其内容将与原始磁盘页中的内容进行合并,
  以返回最新的数据. 当刷写页时, 更新缓冲区内容将与页内容协调然后保存在磁盘上,
  以覆盖原始页. 如果协调后页的大小大于最大值, 则将其拆分为多个页.
  - 更新缓冲区使用跳表 (skiplist) 来实现,
    跳表具有类似于搜索树的复杂度, 但具有更好的并发性.

* 在原地更新的`B`树的实现中, **写放大**是最显著的问题之一:
  对一个`B`树页后续的更新可能需要在每次更新时更新其磁盘驻留页副本.
* 第二个问题是**空间放大**: 我们保留了额外的空间以实现更新.
  这也意味着, 为了传输承载着所请求数据的每个有用字节,
  我们不得不额外传输一些空字节以及页的其余部分.
* 第三个问题是解决**并发问题**和处理闩锁的复杂性.
* 为了同时解决这三个问题, 我们必须采取一种与目前已经讨论过的方法完全不同的方法.
  缓冲更新有助于减轻写放大和空间放大问题, 但不能解决并发问题.

---

* 我们可以通过使用仅追加存储来对不同节点进行批量更新, 将节点链接成链,
  并使用内存数据结构, 该结构允许通过单个`CAS`操作在节点之间建立指针,
  从而使树无锁. 这种方法称为 `Buzzword` 树 (**Bw树**).
* `Bw`树区分基节点的写入与修改. 修改 (增量节点) 形成一个链:
  一个从最新修改到旧修改的链表, 该链表末尾则是基节点 (base node).
  每个更新都可以单独存储, 而不需要重写磁盘上的现有节点.
  增量节点 (delta node) 可以表示插入,
  (与插入无法区分的) 更新或删除.
* 由于基节点和增量节点的大小不太可能是页对齐的, 所以将它们连续存储是有意义的,
  而且由于在更新期间基节点和增量节点都不会被修改
  (所有修改只是将一个节点插入现有链表头部), 所以我们不需要预留任何额外的空间.
* 将节点作为逻辑实体而不是物理实体是一种有趣的范式转变:
  - 我们不需要预先分配空间, 不需要节点具有固定的大小,
    甚至不需要将它们保持在连续的内存段中.
* 这当然有一个缺点: 在读取过程中, 必须遍历所有增量并将其应用到基节点上,
  以重建实际的节点状态. 这有点类似于`LA`树所做的事情:
  - 更新与主结构分开存放, 并在读取时重放.
* 维护一个允许将数据项插到子节点之前的磁盘树状结构的成本相当高:
  它要求我们不断地更新父节点指向最新增量的指针.
  这就是为什么由增量节点和基节点链接在一起而组成的`Bw`树节点具有逻辑标识符,
  并使用一个从标识符到其在磁盘上位置的内存映射表.
* 使用这种映射还帮助我们摆脱闩锁: `Bw`树不是在写入时获取独占所有权,
  而是对映射表中的物理偏移量使用`CAS`原子操作.

* 要更新一个`Bw`树节点, 该算法执行以下步骤:
  - (1) 通过从根到叶遍历树来定位目标逻辑叶节点.
    映射表包含了到更新链中的目标基节点或最新增量节点的虚拟链接.
  - (2) 使用指向在步骤`1`中定位的基节点 (或指向最新的增量节点)
    的指针来创建一个新的增量节点.
  - (3) 用指向步骤`2`期间创建的新增量节点的指针来更新映射表.
  - (4) 步骤`3`中的更新操作可以使用`CAS`这个原子操作来完成,
    因此与指针更新并发的所有读取都被安排在写入之前或之后,
    读取者或写入者都不会被阻塞. 在此之前的读取是沿着旧指针进行的,
    看不到新的增量节点, 因为它尚未被放置.
    而在此之后的读取则是沿着新指针进行的, 可以观察到更新.
    如果两个线程试图将一个新的增量节点放置到同一个逻辑节点,
    那么只有其中一个线程可以成功, 而另一个线程必须重试该操作.

* `Bw`树是一个有趣的`B`树变体, 其在如下几个重要方面进行了改进:
  - **写放大**, 非阻塞访问和缓存友好性.
  - 实验性的存储引擎`Sled`实现了它的一个修改版本.
  - 而`CMU`数据库组开发了`Bw`树的一个内存版本,
    称为`OpenBw`树, 并发布了一个实用的实现指南.

## 日志结构存储

* 不可变`LSM`树 (Log-Structured Merge Tree) 使用仅追加存储和合并协调,
  而`B`树则在磁盘上定位数据记录并在文件中的原始偏移量上更新页.
* 原地更新存储结构针对读取进行了性能优化: 当在磁盘上定位数据之后,
  就可以将记录返回客户端. 这是以牺牲写入性能为代价的:
  - 要在原地更新数据记录, 首先必须在磁盘上进行定位.
* 另一方面, 仅追加存储是对写入性能进行的优化.
  写操作不必在磁盘上找到记录来覆盖它们. 然而, 这是以读取性能为代价的,
  读取必须检索多个数据记录版本并对它们进行协调.

* `LSM`树是类似于`B`树的磁盘驻留结构的变体, 其中节点完全被填满,
  并为顺序磁盘访问进行了优化. 尽管`LSM`树通常被用作`B`树的一种替代,
  但`B`树通常被用作`LSM`树的不可变文件的内部索引结构.
* `LSM`树在写远大于读的应用程序中特别有用. 考虑到数据量和采集速率在不断增长,
  在现代数据密集型系统中这种情况是常见的.
* `B`树和`LSM`树都需要一些内务处理 (housekeeping) 来优化性能,
  但原因不尽相同. 由于分配文件的数量稳定增长, 所以`LSM`树必须合并和重写文件,
  以确保在读取过程中访问尽可能少的文件, 因为请求的数据记录可能分布在多个文件中.
* 另一方面, 可变文件可能必须被部分或全部重写,
  以减少碎片并回收被更新或删除的记录所占用的空间.
  - 当然, 内务处理的确切工作很大程度上取决于具体实现.

* `LSM`树由较小的内存驻留组件和较大的磁盘驻留组件组成.
  要在磁盘上写出不可变的文件内容, 首先需要在内存中对其进行缓冲和排序.
* 内存驻留组件 (通常称为 `memtable`) 是可变的: 它缓冲数据记录,
  并充当读写操作的目标. 当其大小达到一个可配阈值时,
  `memtable`中的内容将会被持久化到磁盘上.
* `memtable`的更新不需要磁盘访问, 也没有相关的`I/O`开销.
  需要一个单独的预写日志文件以保证数据记录的持久性.
  在向客户端确认操作之前, 数据记录会被追加到日志中并提交到内存.
* 缓冲是在内存中完成的: 所有读写操作都应用于一个内存驻留表,
  该表维护一个允许并发访问的有序数据结构, 其通常是某种形式的内存排序树,
  或任何可以提供类似性能和特征的数据结构.
* 磁盘驻留组件则是通过将内存中缓冲的内容刷写到磁盘来构建的.
  磁盘驻留组件仅用于读取: 缓存的内容被持久化成文件,
  而这些文件永远不会被修改.
* 这允许我们从简单操作的角度来考虑: 对内存中的表进行写操作,
  以及对磁盘和基于内存的表进行读, 合并和文件删除操作.
* 在`LSM`树中, 插入, 更新和删除操作不需要在磁盘上定位数据记录.
  但是, 在读取的过程中需要对冗余的记录进行协调.

---

* 由于磁盘驻留表上的内容是经过排序的, 所以我们可以使用多路归并排序算法.
  例如, 我们有三个数据源: 两个磁盘驻留表和一个`memtable`.
  - 通常, 存储引擎提供游标或迭代器来遍历文件内容.
    此游标保存上次消耗的数据记录的偏移量,
    可以用来检查迭代是否完成, 也可以用于抽取下一个数据记录.
* 多路归并排序使用一个优先级队列, 如**最小堆** (`min-heap`),
  该队列最多保存`N`个元素 (其中`N`是迭代器的数目),
  该队列对其内容进行排序并准备返回的下一个最小的元素.
  每个迭代器的头被放入队列. 队列头部的元素就是所有迭代器的最小值.

* 合并迭代只是从多个数据源合并数据的一个方面.
  另一个重要方面是与同一键相关联的数据记录的协调和冲突解决.
* 不同的表可能持有相同键的数据记录, 例如更新和删除,
  这时必须对它们的内容进行协调.
  前例中的优先级队列的实现必须能够允许插入与相同键关联的多个值,
  并触发协调.

* 在`LSM`树中, 磁盘驻留表的数量不断增长, 但可以通过触发周期性的压实来减少.
* 压实会挑选多个磁盘驻留表, 使用前面提到的合并和协调算法迭代它们的全部内容,
  并将结果写入新创建的表.

---

* 在压实过程中, 墓碑不会被立即丢弃. 它们会被保留,
  直到存储引擎可以确定在任何其他表中都不存在时间戳更小且键相同的数据记录.
* **RocksDB**保留墓碑直到它们到达最底层. 这是因为数据库的最终一致性,
  这样做可以确保其他节点观察到墓碑.
  - 在压实过程中保留墓碑对于避免数据复活很重要.
* 压实提供了多种优化机会, 并且有许多不同的压实策略.
  其中一种常用的压实策略称为层级压实 (leveled compaction),
  例如**RocksDB**就使用这一策略.

* 层级压实将磁盘驻留表划分为多个层级. 每个层上的表都有目标大小,
  每个层都有相应的序号 (标识符).
  - 有些违反直觉的是, 序号最高的层被称为最底层.

* 在实现最优压实策略时, 我们必须考虑多个因素. 一种方法是回收重复记录占用的空间,
  减少空间开销, 但这会产生由不断重写表导致的更高的写放大.
  替代方案是避免连续重写数据, 而这又增加了读放大
  (在读取期间协调关联到相同键的数据记录的开销)
  和空间放大 (因为冗余记录会被保存更长时间).

---

* 总结起来, 当以不可变的方式在磁盘上存储数据时, 我们面临三个问题:
  - **读放大**
  - 由为了检索数据而需要读取多个表所引起.
  - **写放大**
  - 由压实过程中不断进行的重写所引起.
  - **空间放大**
  - 由存储关联到同一键的多个记录所引起.

* `RUM`猜想指出, 减少其中两项开销将不可避免地导致第三项开销的恶化,
  并且优化只能以牺牲三个参数中的一个为代价.
* 这个开销模型并不完美, 因为它没有考虑其他重要的指标,
  如延迟, 访问模式, 实现复杂度, 维护成本以及与硬件相关的细节.
  对于分布式数据库很重要的概念, 如一致性含义和复制开销,
  也没有被考虑进去.
* 然而, 该模型可以被用作初步评估或当作一种经验法则,
  它有助于我们理解存储引擎必须提供什么.

* 磁盘驻留表通常使用有序字符串表 (Sorted String Table, `SSTable`) 来实现.
  顾名思义, `SSTable`中的数据记录是按照键顺序进行排序和布局的.
* `SSTable`通常由两个组件组成: 索引文件和数据文件.
  索引文件是用能够以对数时间复杂度 (如`B`树) 或常量时间复杂度
  (如哈希表) 进行查找的某种结构来实现的.

* `LSM`树中读放大的来源是, 我们必须寻址多个磁盘驻留表, 以便完成读取操作.
  这是因为我们不一定能预先知道一个磁盘驻留表是否包含要搜索的键指向的数据记录.
* 防止表查询的方法之一是在元数据中存储其键的范围 (存储给定表中的最小和最大键),
  并检查要搜索的键是否在该表的范围之内. 这一信息是不精确的,
  它只能告诉我们数据记录是否可能会出现在表中.
* 为了改进这种情况, 包括 Apache Cassandra 和 RocksDB
  在内的许多实现都使用一种称为布隆过滤器 (Bloom Filter) 的数据结构.

---

* **布隆过滤器**是一种空间效率很高的概率型数据结构,
  可以用来测试元素是否是集合的成员. 它可能产生假阳性匹配
  (返回"元素在集合中"的结果, 而实际上该元素却不在集合中),
  但不会出现假阴性匹配
  (若返回"元素不在集合中"的结果, 则保证该元素肯定不是集合的成员).
* 换句话说, 可以使用布隆过滤器来**判断**键是否可能在表中或**肯定不在**表中.

* 在查询期间跳过布隆过滤器返回"不匹配"的文件, 而只访问其余文件,
  以查明数据记录是否确实存在.
* 使用与磁盘驻留表相关联的布隆过滤器能显著减少读取过程中要访问的表的数量.

* 布隆过滤器使用一个大的比特数组和多个哈希函数构建.
  将这些哈希函数应用于表中记录的键, 并将哈希值作为数组下标来将其对应比特位设置为`1`.
* 如果哈希函数所确定的所有比特位都为`1`, 则表示该搜索键在该集合中可能是存在的.
  在查找过程中, 当检查布隆过滤器中的元素是否存在时, 需要再次计算键的哈希函数:
  - 如果所有哈希函数确定的位都为`1`, 则返回肯定的结果,
    说明该项目有一定概率是集合中的成员;
  - 如果至少有一个位为`0`, 则我们可以肯定地说该元素不存在于集合中.
* 应用于不同键的哈希函数可能返回相同的比特位并导致哈希冲突,
  比特位为`1`仅表示某个哈希函数为某个键产生了该比特位上的一个置位.
* 假阳性的概率是通过配置比特集的大小和哈希函数的数量来控制的:
  - 在较大的比特集中, 冲突的概率较小;
  - 同样, 若拥有更多的哈希函数, 则我们也可以检查更多的比特位,
    这也将产生一个更精确的结果.

* 较大的比特集占用较多的内存,
  而用较多的哈希函数计算结果又可能会对性能产生负面影响,
  因此我们必须在可接受的概率和产生的开销之间进行权衡.

---

* 随着固态硬盘变得更经济, 日志结构存储 (Log-Structured Storage, LSS)
  系统开始流行. `LSM`树和固态硬盘是一个很好的搭配,
  因为序列化的工作负载和仅追加写入有助于减少原地更新带来的放大,
  而原地更新会对固态硬盘的性能产生负面影响.

* 总结起来,
  在固态硬盘上使用日志结构存储的动机是将小的随机写入缓冲在一起进行批处理以摊销`I/O`成本,
  这通常会减少操作的数量, 进而减少触发垃圾收集的次数.

## 分布式系统简介

* 除了分布式系统中的**时钟同步**非常困难之外, 当前时间也在不断变化:
  你可以从操作系统请求当前的`POSIX`时间戳,
  并在执行几个步骤后请求另一个当前时间戳, 两次结果是不同的.
  - 尽管这是一个明显的现象,
    但是了解时间的来源以及时间戳捕获的确切时刻至关重要.

* `FLP`不可能问题 (`FLP`是作者姓氏的首字母), 论文讨论了一种共识形式:
  - 各进程启动时有一个初始值, 并尝试就新值达成共识.
  - 算法完成后, 所有正常进程上的新值必须相同.
* 如果网络完全可靠, 很容易对特定值达成共识. 但实际上,
  系统容易出现各式各样的故障, 例如:
  消息丢失, 重复, 网络分区, 以及进程缓慢或崩溃.
* 共识协议描述了这样一个系统: 给定初始状态的多个进程,
  它将所有进程带入决定状态.
* 一个正确的共识协议必须具备以下三个属性:
  - **一致性**
  - 协议达成的决定必须是一致的: 每个进程都做出了决定且所有进程决定的值是相同的.
    否则我们就尚未达成共识.
  - **有效性**
  - 达成共识的值必须由某一个参与者提出, 这意味着系统本身不能"提出"值.
  - 这也意味着这个值不是无关紧要 (trivial) 的:
    进程不能总是决定某个预定义的默认值.
  - **终止性**
  - 只有当所有进程都达到决定状态时, 协议才算完成.

* 文献假定处理过程是完全异步的, 进程之间没有共享的时间概念.
  这样的系统中的算法不能基于超时,
  并且一个进程无法确定另一个进程是崩溃了还是仅仅运行太慢.
* 论文表明, 在这些假设下, 不存在任何协议能保证在有限时间内达成共识.
  - 完全异步的共识算法甚至无法容忍一个远程进程无通知地突然崩溃.
* 如果我们不给进程完成算法步骤设定一个时间上限,
  那么就无法可靠地检测出进程故障, 也不存在确定性的共识算法.
* 但是, `FLP`不可能定理并不意味着我们要收拾东西回家
  (由于达成共识是不可能的).
  - 它仅仅意味着我们不能总是在有限的时间内在一个异步系统中达成共识.
  - 实践中, 系统至少会表现出一定程度的**同步性**,
    而要想解决共识问题还需要一个更完善的模型.

* 从`FLP`不可能定理中可以看出**时序假设**是分布式系统的关键特征之一.
  在异步系统中, 我们不知道进程运行的相对速度,
  也不能保证在有限时间内或以特定顺序传递消息.
  - 进程可能要花无限长的时间来响应, 而且无法总是可靠地检测到进程故障.
* 对异步系统的主要批评在于上述假设不切实际:
  进程不可能具有任意不同的处理速度, 链路传递消息的时间也不会无限长.
  - 依赖时间能够简化推理, 并提供时间上限的保证.

* 在异步模型中不一定能解决共识问题. 而且, 不一定能设计出高效的异步算法.
  对于某些任务, 切实可行的解决方案很可能需要依赖时间.

---

* 我们可以放宽一些假设, 认为系统是同步的. 为此我们引入了时间的概念.
  在同步模型下对系统进行推理要容易得多. 它假定各进程的处理速度相近,
  传输延迟是有限的, 并且消息传递不会花任意长的时间.
* **同步系统也可以表示为同步的进程本地时钟**:
  **两个进程本地时间源之间的时间差存在上限**.
* 在同步模型中设计系统可以使用**超时机制**. 我们可以构建更复杂的抽象,
  例如领导者选举, 共识, 故障检测以及基于它们的其他抽象.
  - 这使得最佳情况的场景更加健壮,
    但是如果时序假设不成立则可能导致故障.

* 异步和同步模型的性质可以组合使用, 我们可以将系统视为部分同步的.
  部分同步的系统具有同步系统的某些属性, 但是, 消息传递,
  时钟漂移和相对处理速度的边界范围可能并不精确,
  并且仅在大多数时候成立.
* **同步是分布式系统的基本属性**: 它对性能, 扩展性和一般可解性有影响,
  并且有许多对系统正常工作来说是必要的因素.
  - 本书中讨论的一些算法就工作在同步系统的假设下.

## 故障检测

* 我们可以通过算法的效率来判断其优劣: 故障检测器识别进程故障的速度有多快.
  另一种方法是观察算法的准确性: 是否精确地检测到了进程故障.
* 换句话说,
  如果一个算法错误地认为一个活着的进程发生了故障或者不能检测出实际已发生的故障,
  那么它就是不准确的.
* 我们可以把效率和准确度之间的关系看作是一个可调参数:
  一个更高效的算法可能更不准确, 而一个更准确的算法通常更不高效.
  - 建立既准确又高效的故障检测器被证明是不可能的.

* 许多分布式系统使用心跳 (heartbeat) 来实现故障检测器.
  由于其简单性和很强的完备性, 这种方法非常普遍.
  - 我们在这里讨论的算法假设不存在拜占庭式故障:
  - 进程不会试图故意谎报它们自己及相邻进程的状态.

## 领导者选举

* 尽管领导者选举和分布式锁 (即对共享资源的独占所有权)
  从理论角度看可能很相似, 但它们略有不同.
  如果一个进程因为执行临界区而持有锁, 那么对于其他进程来说,
  知道现在到底是谁持有锁并不重要, 只要满足活动性
  (即锁最终将被释放并允许其他人获得它) 就可以了.
* 相比之下, 选出的进程具有一些特殊性质, 必须让所有其他参与者都知道,
  因此新当选的领导者必须将其角色通知所有对等进程.

## 复制和一致性

* 然而, 由于原子地更新数据的多个副本是一个等同于共识的问题,
  因此数据库中的每个操作都执行共识操作可能成本相当高.
* 我们可以探索一些性价比更高且更灵活的方法,
  允许参与者之间存在某种程度的差异,
  但使数据从用户的角度看起来是一致的.

* 在谈论复制时, 我们最关心这三种事件:
  - `写入`, `副本更新`和`读取`.
* 这些操作触发了由客户端发起的一系列事件. 在某些情况下,
  从客户端角度看, 更新副本可能发生在写操作完成之后,
  但这仍然不能改变这样一个事实:
  - 客户端必须能够以特定的顺序观察到发生过的操作.

* 我们希望在容忍网络分区的同时实现一致性和可用性.
  网络可能被分裂为几个部分, 在这些部分之间, 进程不能相互通信:
  - 被分隔的节点之间发送的一些消息将无法到达目的地.
* 可用性要求任何无故障的节点交付结果, 而一致性要求结果是可线性化的.
* CAP 猜想讨论了一致性 (consistency), 可用性 (availablity)
  和分区容忍性 (partition tolerance) 之间的权衡.

* 构建系统时, 我们可以在提供尽力而为 (best effort)
  可用性的同时保证强一致性, 或者在提供尽力而为一致性的同时保证可用性.
* 在这里, 尽力而为意味着:
  - 如果一切正常, 系统将不会故意违反任何保证,
  - 但是在网络分区的情况下, 允许系统削弱和违反保证.
* 换句话说, CAP 描述了一系列潜在的可选项, 而在这些选项的两端是以下两种系统:
  - 一致性和分区容忍系统 (`CP` 系统)
  - `CP` 系统更倾向拒绝请求, 而不是提供可能不一致的数据.
  - 可用性和分区容忍系统 (`AP` 系统)
  - `AP` 系统放松了一致性要求, 允许在请求期间提供可能不一致的值.

* **PACELC** 猜想是 CAP 的一个扩展, 它指出在网络分区 (P)
  存在的情况下, 可用性和一致性 (AC) 之间存在一个选择.
  - 否则 (Else, E), 即使在没有网络分区的情况下系统运行正常,
  - 我们仍然要在延迟 (Latency, L) 和一致性 (C) 之间做出选择.

* **CAP** 猜想只是一条**经验法则**, 并不一定能说明全部事实.

* CAP 猜想仅以它们最强的形式讨论一致性和可用性:
  - 可线性化和系统最终响应每一个请求的能力.
* 这迫使我们在这两个属性之间做出艰难的权衡. 然而,
  有些应用程序可以从稍微放松的假设中获益,
  我们可以用它们较弱的形式来思考这些属性.
* 系统不一定非得在一致或可用中二选一, 也可以提供更宽松的保证.
  我们可以定义两个可调度量:
  - 收成 (harvest) 和产量 (yield),
  - 在两者之间进行选择仍然可以形成正确的行为:
  - **收成**
  - 收成定义查询的完成程度: 如果查询必须返回`100`行,
    但由于某些节点不可用而只能获取`99`行,
    这仍然比查询完全失败而不返回任何内容要好.
  - **产量**
  - 产量指成功完成的请求数与尝试请求总数之比.
    产量与正常运行时间 (uptime) 不同,
    例如一个繁忙的节点没有宕机,
    但仍然可能无法响应某些请求.
* 这就把权衡的重点从绝对条件变成了相对条件.
  我们可以用收成换取产量, 并允许某些请求返回不完整的数据.
  提高产量的一个方法是只从可用的分区返回查询结果.

---

* **严格一致性** (strict consistency) 相当于完全透明的复制:
  - 任何进程的任何写入都可以立即被任何进程的后续的读操作读取.
  - 它涉及全局时钟的概念, 如果在时刻 `t1` 有 `write(x, 1)`,
    则任何 `read(x)` 的操作将在时刻 `t2 > t1` 时返回新写入的值 `1`.
* 不幸的是, 这只是一个理论模型, 且**不可能实现**,
  因为物理定律和分布式系统的工作方式限制了事情发生的速度.

* **可线性化** (linearizability) 是**最强**的**单对象**,
  **单操作**一致性模型. 该模型下, 在写操作开始和结束之间的某个时间点,
  写操作的效果严格一次性地对所有读取者可见, 没有客户端会观察到状态转移,
  部分 (即未完成的, 仍在进行中的) 或不完全 (即在完成之前中断的) 的写入操作.
* 并发操作表现为可见性属性所支持的可能的顺序历史记录之一.
  可线性化有一定的不确定性, 因为可能存在不止一种方式来对事件进行排序.

* **可线性化**最重要的一个特征是**可见性**: 一旦操作完成,
  每个参与者都必须能看到它, 并且系统不能 `"穿越到过去"`
  还原它或使它对某些参与者不可见.
  - 换句话说, 可线性化禁止读取过时的数据, 它要求读取是单调的.
* 这种一致性模型最好用原子的 (即: 不间断, 不可分割) 操作来解释.
  操作不一定是瞬时的 (也因为并不存在瞬时的东西),
  但是它们的效果必须在某个时间点变得可见,
  从而造成操作是瞬时的错觉. 这个时刻称为**线性化点** (linearization point).

* 有趣的是, 可线性化在其传统理解中被视为局部性质,
  并且意味着其是独立实现和验证的单元组合.
  合并可线性化的历史将产生一个同样可线性化的历史.
* 换句话说, 其中所有对象都是可线性化的系统, 也是可线性化的.
  这是一个非常有用的属性, 但是我们应该记住, 它的范围仅限于单个对象,
  而且即使对两个独立对象的操作是可线性化的,
  涉及两个对象的操作还必须依赖于额外的同步手段.

---

* **顺序一致性** (sequential consistency) 允许对操作进行排序,
  就好像它们是以某种串行顺序执行的一样,
  并要求属于某一进程的操作在排序后保持先后顺序不变
  (不同进程的操作之间, 先后顺序可能改变).
* 进程可以观察到其他参与者执行操作的顺序与它们自己的历史相一致,
  但是从全局角度看, 这个视图可能会过时任意长时间.
  进程之间的执行顺序是未定义的, 因为这里没有共享的时间概念.

* 每个进程都可以按照自己的程序指定的顺序发出读写请求,
  这是很符合直觉的, 因为任何非并发的单线程程序都一个接一个地执行它的步骤.
  从同一进程传播的所有写操作都按此进程提交它们的顺序出现.
  来自不同来源的操作可以任意排序, 但从读取者的角度来看, 这个顺序是一致的.

* 顺序一致性常与可线性化混淆, 因为两者具有相似的语义.
  顺序一致性与可线性化一样, 要求操作的全局有序性,
  而可线性化要求每个过程的局部有序性与全局有序性一致.
* 换句话说, 可线性化遵循操作的真实顺序. 而在顺序一致性条件下,
  顺序仅适用于来源相同的写入. 另一个重要的区别在于组合:
  - 我们可以组合可线性化的历史,
    并且仍然期望结果是可线性化的,
    而顺序一致的调度是不可组合的.

* 读到过时值 (stale read) 可以用副本差异来解释, 例如:
  - 即使写入以相同的顺序传播到不同的副本, 它们也可能在不同的时间到达.
* 顺序一致性与可线性化的主要区别在于没有全局强制的时间界限.
  在可线性化中, 一个操作必须在其挂钟时间范围内变得有效.
* 当写入操作 `W1` 完成时, 它的结果必须已经被应用,
  并且每个读取者应该能够看到至少与 `W1` 所写入的值一样新的值.
* 类似地, 在一个读操作 `R1` 返回之后, 在它之后执行的任何读操作都应该返回
  `R1` 已经看到的值或更晚的值 (当然, 这也必须遵循同样的规则).
* 顺序一致性放宽了这一要求: 操作的结果可以在操作完成后才变得可见,
  只要从单个参与者的角度来看顺序是一致的.
  同一来源的写操作程序不能互相"跳过":
  - 它们的程序顺序 (相对于自己的执行进程) 必须保持不变.
    另一个限制是操作出现的顺序必须对所有读取者一致.

---

* 尽管通常没有必要使用全局操作顺序, 但在某些操作之间建立顺序可能是必要的.
  在因果一致性 (causal consistency) 模型下,
  所有过程都必须以相同的顺序看到因果相关的操作.
* 没有因果关系的并发写入可以被不同的进程以不同的顺序观察到.

* 这一保证是: 单调读, 单调写, 读自己写 (read-your-write),
  读后写 (write-follow-read).

* 因果一致性可以使用逻辑时钟 (logical clock) 来实现,
  在每个消息中发送上下文元数据, 并总结哪些操作在逻辑上先于当前操作.
  当从服务器接收到更新时, 更新包含了上下文的最新版本.
* 任何操作只有在其前面的所有操作都已应用的情况下才能被处理.
  上下文不匹配的消息将缓存在服务器上, 因为传递这些消息还为时过早.

* **向量时钟** (vector clock) 是一种用于在事件之间建立偏序,
  检测和解决事件链之间分歧的结构. 利用向量时钟,
  我们可以模拟公共时间和全局状态, 将异步事件表示为同步事件.
* 进程维护逻辑时钟的向量, 其中每个时钟对应一个进程.
  每个时钟从初始值开始, 每次新事件到达 (例如发生写入) 时递增.
  当从其他进程接收到时钟向量时,
  进程将其本地向量中的各个值更新为接收到向量中对应进程时钟的最大值
  (即, 传输节点曾经看到的最大时钟值).
* 为了使用向量时钟来解决冲突, 每当我们对数据库进行写入时,
  首先检查写入键的值是否已经在本地存在. 如果前值已经存在,
  我们将一个新版本追加到版本向量中, 从而建立两个写入之间的因果关系.
  否则, 我们启动一个新的事件链, 并用单个版本初始化该值.

* 为了实现因果一致性, 我们必须存储因果关系历史, 支持垃圾收集,
  并要求用户在冲突的情况下解决分歧. 而向量时钟可以告诉你冲突已经发生,
  但并不提出确切的解决方法, 因为解决语义通常是应用程序特定的.
* 因此, 一些最终一致的数据库, 例如 Apache Cassandra,
  不会对操作进行因果排序, 而是使用"最后写胜出"规则来解决冲突.

---

* **最终**是描述值传播的一个有趣的术语, 因为它没有指定它必须发生的硬性时间限制.
  如果传递服务仅仅提供了一个"最终"的保证, 这听起来就很不可靠.
  - 然而在实践中, 这个模型工作得很好, 当下许多数据库都是最终一致的.

* 从服务器端的角度来看, 最终一致的系统通常实现可调一致性,
  其中使用三个变量调节数据的`复制`, `读取`和`写入`:
  - 复制因子 `N`
  - 储存数据副本的节点数.
  - 写入一致性 `W`
  - 使写入成功需要确认的节点数.
  - 读取一致性 `R`
  - 使读取成功需要响应的节点数.
* 通过选择一致性级别使之满足 `R + W > N`,
  系统可以保证返回最近写入的值, 因为读取集合和写入集合之间总是存在重叠.
* 读取时, 至少需要有三个副本中的两个来处理请求, 才能回复一致的结果.
  任何一组节点都将至少包含一个具有给定键的最新记录的节点.

* 当执行写操作时, 协调者应该将其提交给 `N` 个节点,
  但是在继续操作之前只等待 `W` 个节点
  (或者在协调者也是副本的情况下等待 `W-1` 个节点).
* 其余的写操作可以异步完成或失败. 类似地, 当执行读取时,
  协调者必须收集至少 `R` 个响应. 某些数据库使用推测执行
  (speculative execution) 提交额外的读取请求以减少协调者响应延迟.
  - 这意味着如果最初提交的读请求中的一个失败或到达缓慢,
    推测请求可以被计入 `R`.
* 写入较多的系统有时可能选择 `W=1`, `R=N`,
  这允许写操作仅由一个节点确认, 但也会要求所有副本
  (甚至可能出现故障的副本) 都可用于读操作.
* 对于 `W=N`, `R=1` 的组合也是如此:
  - 写入必须应用到所有副本才算成功, 因此便可以从任何节点读取最新值.
* 增加读或写一致性级别会增加延迟, 并提高请求期间对节点可用性的要求.
  而降低一致性级别则可以提高系统的可用性, 同时牺牲一致性.

---

* **Quorum**: 网络分区或节点故障的情况下, 在 `2f+1` 个节点的系统中,
  如果不可用的节点数不大于 `f`, 则活动节点还可以继续接受写入或读取.
  - 换句话说, 这样的系统最多能容忍 `f` 个节点故障.
* 当使用 **Quorum** 执行读写操作时, 系统不能容忍多数派节点的故障.
  - 例如, 如果总共有三个副本, 其中两个已宕机,
    则读写操作将无法达到读写一致性所需的节点数,
    因为三个节点中只有一个节点能够响应请求.
* 在不完全写入的情况下, 使用 `Quorum` 进行读写并不能保证单调性.
  如果在向三个副本中的一个副本写入值之后, 某个写操作失败,
  则 `Quorum` 读取可能返回不完全操作的结果或旧值,
  具体取决于所联系的副本.
* 由于后续的对相同值的读取不需要联系同一个副本,
  因此它们返回的值可能交替出现两种结果. 为了实现读单调性
  (在牺牲可用性的情况下), 我们需要使用阻塞读修复.

* 在读取一致性上使用 `Quorum` 能够提高可用性:
  即使某些节点宕机, 数据库系统仍然可以接受读取和写入.
  要求多数派参与保证了在任意多数派集合中至少有一个节点是重叠的,
  因此任何 `Quorum` 读取都将观察到最近完成的 `Quorum` 写入操作.
* 但是, 使用复制和多数派会增加存储成本:
  我们必须在每个副本上储存数据的一份拷贝. 如果我们的复制因子是 `5`,
  则我们必须存储 `5` 个拷贝.
* 我们可以通过使用见证者副本 (witness replica) 来降低存储成本.
  我们不需要在每个副本上存储数据记录的拷贝,
  而是可以将副本分为拷贝副本 (copy replica) 和见证者副本.
* 拷贝副本仍然持有以前的数据记录. 在正常操作下, 见证者副本仅储存一些记录,
  表示写操作发生过的事实. 然而, 当拷贝副本的数量太少时, 可能会发生一种情况,
  - 例如我们有三个拷贝副本和两个见证者副本, 此时两个拷贝副本宕机,
    我们最终的 `Quorum` 是一个拷贝副本和两个见证者副本.
* 而在写入超时或拷贝副本发生故障的情况下, 为了临时存储记录,
  可以升级见证者副本以临时代替故障或超时的拷贝副本.
  一旦原来的拷贝副本恢复, 升级的副本就可以回退到它以前的状态,
  或者也可以让恢复后的副本成为见证者副本.

* 更一般地, 只要我们遵循如下两个规则,
  那么 `n` 个拷贝副本和 `m` 个见证者副本就能达到与
  `n+m` 个副本相同的可用性保证:
  - 使用多数派 (即 `N/2+1` 个参与者) 执行读和写操作.
  - 该 `Quorum` 中至少有一个副本是拷贝副本.
* 能这么做的原因是数据一定要么在拷贝副本上, 要么在见证者副本上.
  在发生故障时, 修复机制会更新拷贝副本, 在此期间数据储存在见证者副本上.

---

* **CRDT**是一种特殊的数据结构, 它排除了冲突的存在,
  允许以任意顺序应用对它的操作而不改变结果. 这个特性在分布式系统中非常有用.
* 这样的特性使得 `CRDT` 在最终一致性系统中很有用,
  因为它允许系统中的副本状态临时出现分歧.
* `CRDT` 为我们提供了相当多的可能性,
  我们看到越来越多的数据存储利用它来提供强最终一致性 (SEC).
  - 这是一个强有力的概念, 我们可以将其添加到构建容错分布式系统的工具库中.

## 反熵和传播

* 为了保持各节点的同步, 反熵会触发一个后台或前台进程,
  比较和调和丢失或冲突的记录. **后台反熵**进程会利用 `Merkle`
  树之类的辅助数据结构, 从更新日志中识别出分歧的数据.
* **前台反熵**进程会捎带 (`piggyback`) 地在读取或写入请求上附加额外逻辑,
  比如提示移交, 读修复等.
* 如果多副本系统中的副本间出现分歧, 为了恢复一致性和同步,
  我们必须通过成对比较副本状态来查找和修复丢失的记录.
  对于大型数据集这会非常耗时:
  - 我们必须从两个节点分别读出整个数据集, 并将尚未传播的,
    包含最新状态的更改通知相关副本.
  - 为了降低成本, 我们可以考虑副本过期的方式以及数据访问的模式.

* 由于读修复只能修复当前被查询到的数据的不一致性,
  我们需要另一套机制来寻找和修复未被查询到的数据的不一致性.
* 正如我们之前所讨论的, 要想精确找出副本之间哪些行存在不一致,
  需要成对地交换和比较数据记录. 这是很昂贵且不切实际的.
  许多数据库使用 `Merkle` 树来降低数据比对的成本.
* 由于 `Merkle` 树是自底向上递归计算的,
  因此一个数据的变化会触发整个子树的重新计算.
  在树的大小 (决定了交换消息的大小) 和它的精度
  (数据范围有多小, 或者说有多精确) 之间也存在一个权衡.

* 关于反熵的最新研究中引入了位图版本向量 (bitmap version vector)
  来解决数据冲突: 每个节点都会保存各个对等节点的操作日志,
  这些日志包含本地发生或从副本复制过来的事件.
* 反熵时, 我们会比较日志, 并将丢失的数据复制到目标节点.
  每次写入 (由某一个节点协调) 均表示为点 `(i, n)`:
  - 由节点 `n` 协调的节点本地序列号为 `i` 的一个事件.
  - 序列号 `i` 从 `1` 开始, 在每次节点执行写操作时递增.
  - 为了跟踪副本状态, 我们使用节点本地逻辑时钟.
    每个时钟代表一组点的集合, 表示该节点直接看到
    (由该节点本身协调的) 或间接看到
    (由其他节点协调并复制过来的) 的写入.
* 在节点的逻辑时钟中, 节点本身协调的事件之间没有间隙.
  如果某些写操作没有从其他节点复制过来, 时钟则会包含间隙.
  为了让两个节点重新同步, 可以让它们交换逻辑时钟,
  识别出缺失的点所表示的间隙, 然后复制与之相关联的数据记录.
* 为此, 我们需要重新构建出每个点引用的数据记录.
  这项信息存放在点因果容器 (Dotted Causal Container, DCC) 中,
  它将各点映射到给定键上的因果信息.
  - 这样一来, 冲突解决过程就能够获取写操作之间的因果关系.

---

* 进程定期随机选择 `f` 个对等节点并与它们交换当前的"热"信息,
  其中 `f` 是可配置的参数, 称为扇出 (fanout).
  每当进程从其他对等节点获悉新信息时, 它会尝试将其传递到更多节点.
  由于对等节点的选择是概率性的, 因此总会有一些重叠, 消息会被重复传递,
  并且可能会继续流传一段时间. 消息冗余性是一个衡量重复传递开销的度量指标.
  - **冗余性**是一种重要的属性, 它对于 `Gossip` 至关重要.

* 达到收敛 (停止 `Gossip` 过程) 和将消息传递给所有对等节点这两个概念存在细微的差异,
  因为消息可能会在很短时间内就通知到所有对等节点, 但 `Gossip` 仍在继续.
  扇出和延迟取决于系统规模:
  - 在更大规模的系统中, 我们要么增加扇出以保持延迟的稳定,
    要么允许更高的延迟.

* **兴趣损失**可以概率性地计算 (每个进程的每一步都会计算传播停止的概率),
  也可以使用一个阈值 (对接收到重复消息的次数进行计数, 当次数过高时停止传播).
  两种方法都必须考虑集群的规模和扇出.
  对重复消息进行计数以衡量收敛性可以改善延迟并减少冗余.

* 两种方法的折中是在 `Gossip` 系统中构建一个临时的固定拓扑.
  为此, 我们可以创建对等节点间的覆盖网络 (`overlay network`):
  - 节点可以对它的对等节点进行采样, 并根据接近程度
    (通常由延迟来衡量) 选择最佳的联系点.
* 系统中的节点可以构成生成树 (`spanning tree`):
  - 不含重边的无向无环图, 并覆盖整个网络.
  - 有了这个图, 消息就可以按照固定数量的步骤分发.
* 这种方法的潜在缺点之一是, 它可能会导致形成对等节点的"岛".
  - 这些节点之间互相连接, 并且彼此之间具有较强的偏好.
* 为了保持较少的消息数量, 同时在连接断开时能够快速恢复,
  我们可以在系统处于稳定状态时混合使用两种方法 (固定拓扑和基于树的广播),
  而当故障切换和系统恢复时回退到 `Gossip`.

* **Plumtree** 的工作原理是构建出节点的生成树覆盖网络,
  从而以最小的开销主动分发消息. 正常情况下,
  节点将完整的消息发送给一个很小的对等节点子集, 该子集由对等节点采样服务提供.
* 每个节点将完整的消息发送给很小的节点子集, 而对于其余节点,
  它只是惰性地 (`lazily`) 转发消息 `ID`. 如果节点接收到它从未见过的消息标识符,
  它可以查询对等节点以获取这条消息.
* 这个惰性推送 (`lazy-push`) 步骤能确保高可靠性, 并提供一种快速修复广播树的方法.
  故障发生时, 协议通过惰性推送步骤回退到 `Gossip`, 广播消息并修复覆盖网络.

* **HyParView** 和 **Plumtree** 使用混合 `Gossip` 方法:
  - 使用一小部分对等节点来广播消息, 但是在发生故障或网络分区时回退到更宽的对等网络形式.
    两种系统都不依赖所有节点的全局视图, 这是很有用的特性.
  - 不仅因为系统中的节点数量很多 (多数情况下并非这种情况),
    也是因为维护每个节点上的全局成员列表代价很高.
  - 局部视图允许节点仅与相邻节点的一个较小子集保持活动连接.

## 分布式事务

* 事务的**原子性**意味着: 要么它的全部结果都变得可见, 要么全都不可见.
  - 为了保证原子性, 事务必须是**可恢复**的.
* 原子性要求不仅适用于本地操作, 还适用于在其他节点上执行的操作:
  事务的修改必须要么持久化地传播到事务涉及的所有节点, 要么一个都不传播.

* 原子提交试图解决的问题就是要对以下问题达成共识:
  - 是否要执行当前被提议的事务?
  - 事务参与者不能选择, 影响或修改被提议的事务, 更不能提出另一个事务,
    它们只能对自己是否愿意执行此事务进行投票.
* 原子提交算法没有对**准备** (`prepare`), **提交** (`commit`)
  和**回滚** (`rollback`) 这些操作做出严格的语义限制, 数据库开发者需要决定:
  - 何时可以认为数据已准备好提交,
    这时只要交换一个指针便可以让修改对外可见.
  - 如何执行提交本身, 以让事务结果在最短的时间内变得可见.
  - 如果算法决定不提交, 如何回滚事务所做的更改.

* 两阶段提交 (`2PC`) 是数据库事务中的一个重要概念. `2PC` 分为两个阶段:
  - 第一阶段中分发决定的值并收集投票;
  - 第二阶段, 节点仅仅翻转开关, 让第一阶段的结果变得可见.

* 顾名思义, 两阶段提交的执行分为两个步骤:
  - **准备** (`prepare`) 阶段
  - 协调者通过发送 `Propose` 消息告诉参与者关于新事务的提议.
    参与者要决定它们是否可以提交自己那部分事务. 如果参与者决定可以提交,
    就向协调者投赞成票.
  - 否则, 它会要求协调者中止事务. 所有参与者的决定都保留在协调者的日志中,
    并且每个参与者也会在本地保留一份它的决定.
  - **提交** (`commit`) / **中止** (`abort`) 阶段
  - 事务中的操作可以修改多个分区上的状态 (每个参与者代表一个分区).
    但凡有一个参与者投票中止事务, 则协调者也会向所有的参与者发送 `Abort` 消息.
  - 仅当所有参与者都投赞成票时, 协调者才会向他们发送最终的 `Commit` 消息.

* 每个步骤中, 协调者和参与者都必须将各个操作的结果写入持久性存储中,
  以便能够在发生本地故障的情况下重建状态并恢复,
  并且可以将结果转发给其他参与者使其能够重放操作.

* `2PC` 的核心思想在于参与者的承诺: 一旦对提案做出了肯定的回应,
  它不能再反悔, 因此只有协调者才能中止事务.

* 这意味着在发生协调者永久性故障的情况下, 参与者将无法得知最终决定.
  因为这个特性, 我们说 `2PC` 是一种阻塞原子提交算法.
  如果协调者始终无法恢复, 它的替代者只能再次为给定事务收集投票,
  并做出最终决定.

* 为了让原子提交协议在协调者故障下保持健壮并避免进入未决状态,
  三阶段提交 (`3PC`) 协议增加了一个额外的步骤, 并且双方都具有超时机制,
  使得参与者在协调者发生故障时仍能继续提交或中止 (取决于系统状态).
  - `3PC` 假定同步网络模型, 且不存在通信故障.
* `3PC` 在 `提交/中止` 步骤之前添加了一个准备阶段,
  该阶段中协调者告知参与者在提议阶段收集的投票信息,
  即使协调者发生故障, 协议也可以继续执行.
  - `3PC` 的所有其他性质都和 `2PC` 类似, 包括要求有一个协调者.
  - `3PC` 的另一个有用的补充是参与者侧的超时,
    参与者根据进程当前正在执行的步骤,
    超时之时将强制进行提交或中止.

* 三阶段提交包含以下三个步骤:
  - **提议** (`propose`) 阶段
  - 协调者发出提议值并收集投票.
  - **准备** (`prepare`) 阶段
  - 协调者将投票结果通知参与者.
  - 如果投票通过并且所有参与者都决定要提交,
    则协调者会发送一条 `Prepare` 消息, 指示它们准备提交.
    否则, 将发送 `Abort` 消息并退出流程.
  - **提交** (`commit`) 阶段
  - 协调者通知参与者提交事务.

* 一旦所有参与者成功进入已准备状态并且协调者收到了它们的准备确认,
  无论其中任何一方发生故障, 事务都将被提交.
  - 之所以可以这样做是因为此阶段中所有参与者看到的状态是相同的.
  - 在提交阶段, 协调者将准备阶段的结果传达给所有参与者,
    重置它们的超时计数器并完成事务.

* `3PC` 的最坏场景是网络分区.
  - 一些节点已成功进入准备状态, 在超时后将会继续进行提交.
  - 有些节点无法与协调者通信, 因此会在超时后中止.
* 这导致了脑裂:
  - 根据协议, 一些节点会继续进行提交, 另一些节点会中止,
    使得各个参与者处于不一致且矛盾的状态.
* 虽然从理论上说 `3PC` 确实在某种程度上可以解决 `2PC` 的阻塞问题,
  但却带来了更大的消息开销与潜在的不一致性,
  并且在出现网络分区的情况下无法很好地工作.
  - 这或许是 `3PC` 未被广泛使用的主要原因.

---

* 快照隔离保证事务内的所有读取结果与数据库的某个快照一致.
  快照中包含了在事务的开始时间戳之前提交的所有值.
  - 如果存在`写-写`冲突 (即, 两个并发执行的事务尝试写入同一单元格),
    那么只有一个能够提交成功. 这种策略通常称为首个提交者胜利
    (first committer wins).
* 快照隔离能避免读偏斜 (read skew): 一种在读已提交隔离级别下允许的异常.
  - 例如, `x`, `y` 之和应该等于 `100`.
    事务 `T1` 执行操作 `read(x)` 读到值 `70`.
    `T2` 更新两个值 `write(x, 50)` 和 `write(y, 50)` 并提交.
    如果 `T1` 尝试运行 `read(y)`, 并根据 `T2` 新提交的 `y` 的值 `(50)`
    继续执行事务, 将会导致不一致.
  - `T1` 在 `T2` 提交之前读到的值 `x` 与新的 `y` 值彼此不一致.
    而快照隔离仅会让小于某个特定的时间戳的值对事务可见,
    新的 `y` 值对 `T1` 不可见.

* 快照隔离有几个实用的特性:
  - 它只允许对已提交数据的可重复读取.
  - 值是一致的, 因为它们是从某个时间戳的快照中读取的.
  - 冲突的写入将被中止并重试, 以防止产生不一致.
* 尽管如此, 快照隔离下的操作历史不是可串行化的.
  由于只有对相同单元格的冲突写入会被中止, 因此仍可能发生写偏斜 (write skew).
* 写偏斜发生在两个事务修改的值集合不相交时, 每个事务本身的写入都不违反约束.
  两个事务都能提交, 但两个事务的写入合在一起就会违反这些约束.
* 快照隔离提供的语义可以满足许多应用程序的需求, 其主要优势在于读的效率较高,
  因为不需要加锁, 快照数据是无法修改的.

---

* 约束融合性 (Invariant Confluence) 定义为这样一种属性:
  - 两个满足约束但存在分歧的数据库状态一定能够合并为一个有效的最终状态.
  - 这里说的约束对应 `ACID` 中的一致性.

* 允许协调避免的系统模型必须保证以下性质:
  - __全局有效性__
  - 无论对于合并后的状态, 还是刚提交的存在分歧的状态,
    所需的约束条件始终是满足的, 事务不会观察到无效的状态.
  - __可用性__
  - 如果所有包含状态的节点对客户端都是可达的, 那么事务必然会成功提交,
    除非提交将违反某个事务约束, 这种情况下事务会中止.
  - __收敛__
  - 节点可以独立维护其本地状态, 但如果之后没有新的事务并且网络分区不会无限持续下去,
    它们一定能够达到相同的状态.
  - __不需协调__
  - 本地事务的执行独立于为其他节点执行的对本地状态的操作.

* 一个实现协调避免的例子是**读原子性多分区** (Read-Atomic Multi Partition, RAMP) 事务.
  __RAMP__ 使用多版本并发控制和当前进行中操作的元数据来从其他节点获取缺失的状态更新,
  从而允许读取和写入操作能够并发进行.
  - 例如, 如果读取操作与某些修改相同的条目的写入操作存在重叠,
    这种情况可以被检测出来, 必要时, 还可以通过一轮额外的通信,
    利用正在进行的写入操作的元数据获取所需的信息来对其进行修复.
* 在分布式环境下使用基于锁的方法可能不是个好主意, 相反, __RAMP__ 提供以下两个性质:
  - __同步独立性__
  - 一个客户端的事务不会阻塞, 中止或是强迫另一个客户端的事务等待.
  - __分区独立性__
  - 客户端无须联系那些事务不涉及的分区.
* __RAMP__ 引入了读原子性 (read atomic) 隔离级别:
  - 事务不会观察到来自进行中的, 未提交的或中止事务的正在进行的状态变更.
  - 换句话说, 事务更新要么全都对并发事务可见, 要么全都不可见.
  - 根据该定义, 读原子性隔离级别还排除了断裂读 (fractured read):
  - 即事务仅观察到其他事务执行的写入的一个子集.
* __RAMP__ 提供原子的写可见性, 而不需要互斥
  - 其他解决方案 (例如分布式锁) 往往将这两者耦合在一起.
  - 这意味着事务不会相互阻塞.

* __RAMP__ 分发事务元数据, 这些元数据允许读取操作检测到并发进行的写入.
  通过元数据, 事务可以检测到较新的记录版本, 找到并获取最新的记录版本,
  然后对它们进行操作. 为了避免协调, 所有本地的提交决定也必须在全局范围内生效.
* 在 __RAMP__ 中, 这是通过以下要求解决的:
  - 当某个分区中的写入变为可见时, 此事务中所有其他分区的写入也必须可见.
* 为了使得读写操作不阻塞其他并发的读写操作, 同时在本地和整个系统范围内
  (事务修改过的所有其他分区中) 都保持读原子性隔离级别,
  __RAMP__ 使用两阶段提交来处理写入操作:
  - **准备阶段**
  - 第一阶段准备写操作并将其放到相应的目标分区中, 但不使其对外可见.
  - **提交/中止阶段**
  - 第二阶段将事务的写操作进行的状态更改变得可见,
    使其在所有分区上原子地变得可用, 或者回滚更改.
* __RAMP__ 允许一个记录同时存在多个版本:
  - 最新值, 进行中的未提交更改, 以及被后来的事务覆盖掉的过期版本.
    之所以需要保留过期版本, 仅仅是为了正在进行中的读取请求.
    只要所有并发的读操作结束, 就可以丢弃过期的值.
* 为了防止, 检测, 避免并发操作之间的冲突, 往往需要引入协调开销, 因此,
  想让分布式事务高效且可扩展并不容易.
  - 系统越大, 承载的事务越多, 产生的协调开销也就越大.
  - 本节中描述的方法尝试用约束条件来确定可以避免协调的地方,
    从而减少协调的量, 只在绝对必要时才付出全部的代价.

## 共识

- 理论上说, 共识算法具有三个性质:
  - __一致性__
  - 所有正确进程决定的值都相同.
  - __有效性__
  - 决定的值是由其中一个进程提议的.
  - __终止性__
  - 所有正确进程最终都会做出决定.

- 可靠广播能确保各个进程对传递的消息达成一致,
  而原子广播还可以确保它们对消息的顺序也达成一致
  (即对每个接收者来说, 消息传递的顺序都相同).

- 总结一下, 原子广播必须保证两个基本性质:
  - __原子性__
  - 各个进程必须就接收到的消息集合达成一致.
    所有无故障的进程要么全都收到了消息, 要么全都没收到.
  - __有序性__
  - 所有无故障的进程均以相同的顺序收到消息.
- 因此, 这里消息的传递是原子的: 每条消息要么传递给了所有进程,
  要么不传递给任何进程; 如果消息被传递了,
  那么其他消息要么位于其之前, 要么位于其之后.

---

- 在诞生后的十多年间, `Paxos` 几乎是共识算法的代名词,
  但在分布式系统社区中, 它一直以难以理解著称. `2013`年,
  一种称为 `Raft` 的新算法出现了,
  设计它的研究者希望创造一种易于理解和实现的算法.

- `Raft` 将**领导者**的概念变成头等公民, 以此来简化共识问题.
  领导者负责协调状态机的操作和复制.

- `Raft` 中的每个参与者都可以扮演以下三个角色之一:
  - __候选者__ (`candidate`)
  - 领导者的位置是暂时的, 任何参与者都可以担任此角色.
    要成为领导者, 节点必须先变成候选者, 并尝试收集多数派的投票.
    如果候选者既没有赢得选举也没有输掉选举
    (票数被多个候选者平分, 谁也没有得到多数派的投票),
    那么将开启新的任期并重新选举.
  - __领导者__ (`leader`)
  - 当前的临时集群领导者, 负责处理客户端请求并与复制状态机进行交互.
    领导者当选的时间段称为任期 (`term`). 每个任期用一个单调递增的数字来标识,
    它可以持续任意长的时间段. 如果当前的领导者崩溃, 无响应或其他进程怀疑它发生了故障
    (可能是由于网络分区或消息延迟导致的), 则会选举新的领导者.
  - __跟随者__ (`follower`)
  - 被动参与者, 负责保存日志条目以及响应领导者和候选者的请求.
    `Raft` 的跟随者角色类似于 `Paxos` 中的接受者加学习者.
    所有进程一开始都是跟随者.

- 为了在不依赖时钟同步的情况下保证全局偏序关系,
  `Raft` 将时间分为任期 (也称为 `epoch`),
  每个任期内领导者是唯一且稳定的. 任期用单调递增的数字编号,
  每个命令用任期编号和任期内的消息编号唯一标识.
